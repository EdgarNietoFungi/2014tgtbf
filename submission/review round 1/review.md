Dear Mr Chris H.J. Hartgerink,

Thank you for your submission to Collabra. I was able to get one reviewer
with expertise in quantitative methods, and this reviewer did an outstanding
job. I also independently read the manuscript before consulting this
review. Both of us found much to like in your manuscript, and identified
some issues that need to be addressed. Thus, I invite you to submit a
revised version of this paper for further consideration at Collabra.

The reviewer did an outstanding job of articulating their concerns with the
paper and I will not summarize all of their points in my letter. I would
like you to address all of their concerns in the letter of response and if
possible in the revised paper. Below I will add a few points of my own.

First, your arguments rest on the assumption that authors and/or readers are
interpreting the non-significant results in the papers as evidence for the
null. There are several alternative possibilities. One is that some of
these null results may actually be interpreted as evidence against the null,
particularly those that are marginally significant. Second, authors and
readers may sometimes acknowledge that their non-significant results are
inconclusive and may explicitly resist treating them as evidence for the
null. Third, they may be ignored altogether (though I admit this is less
likely since you are not using non-significant results that are only
reported in tables, and only using ones that are reported in APA style in
the text). This is not necessarily a problem for all of your points, but it
does matter for the more conceptual argument about the potential harm that
false negatives do to our field. Thus, I would caution you against assuming
that non-significant results are always interpreted as evidence for the
null. You may also consider excluding results with p-values between .05 and
.10 from your analyses, since these are especially likely to have been
interpreted as evidence against the null. I don’t have a strong feeling
about this last suggestion – what’s more important to me is that you
avoid making a strong assumption about how authors and readers are
interpreting these non-significant findings (except in cases where you coded
this, as perhaps in application 2).

Regarding the analyses of RP:P, you make an important point about the fact
that many of the non-significant results are inconclusive, and you also
acknowledge that the authors of the RP:P themselves point this out.
However, I think it is somewhat of a straw man to criticize the RP:P for not
having enough power to differentiate between a null result and a true effect
of r = .10. I don’t think many people think that the RP:P was intended to
have that kind of power/precision. Moreover, your own test has very low
precision for testing this (since the confidence interval goes all the way
from 0 to 100% of non-significant findings being potential false negatives).
Thus, I would suggest focusing more on the analyses examining the
possibility that some of the RP:P non-significant findings are actually
false negatives of medium or large effects. Here, you can rule out more
possibilities (i.e., you can say that your analyses suggest there are fewer
than 21 medium effects that were non-significant in the RP:P replications,
etc.). Perhaps this is my bias showing through, but I also thought that a
few sentences in this section were overstatements. Specifically, you write
that the RP:P results “say hardly anything about whether there are truly
no effects” (p. 19) – I agree that this statement is correct, but I
think it places a lot of emphasis on distinguishing between null and very
small effects, so holds the RP:P to a very high standard. That’s fair
enough, but it may not be clear to readers that this is the standard you are
using. Similarly, you also write “any conclusions on the validity of
individual effects based on “failed” replications, as determined by
statistical significance, is unwarranted and irresponsible.” First, the
word “irresponsible” implies that this is a common interpretation (i.e.,
that people are doing this irresponsible thing) – is it? Second, I am not
familiar with every single result in the RP:P, but is it really the case
that none of the individual results were strong enough to warrant some kind
of conclusion on the validity of that individual effect? I could imagine
that some of the studies had sufficient precision to draw some conclusion
based on a non-significant result. I am nitpicking here, and I will admit
that I am probably more sensitive than the average reader to statements that
could easily be misinterpreted, so feel free to take these suggestions or
leave them.

On page 21, you discuss “potential explanations” for the lack of change
in power/sample size in the field over time. One piece missing from your
explanation, in my opinion, is p-hacking. P-hacking lets us get significant
results at a high rate even when our sample sizes suggest that our studies
should be underpowered. Likewise, in the paragraph beginning “Reducing
the emphasis on binary”, you also do not mention reducing p-hacking as
another route to improving the situation. Transparency/disclosure
requirements would go a long way to preventing people from being able to
p-hack their way to significant results using small samples. If they were no
longer able to do that, they would be required to get larger samples in
order to detect the phenomena they’re studying. Again, feel free to take
or leave this suggestion, but it stood out to me as an important missing
piece of the puzzle in this section.

Smaller points:
-On page 10, you write “because effect size […] are typically
overestimated population effect sizes” – is this true even for non-focal
tests?
-On page 13, you write the mean effect is r = .257 in 1985 and r = .187 in
2013 – what are these numbers referring to?
-Figure 4 – I wonder if this would be more useful as a percentage of all
papers, not as a percentage of papers that report a non-significant result?
Both are interesting, I’m not sure which one readers will find more
interesting.
-Figure 5 – how confident are you that degrees of freedom are a good proxy
for N?
-What does the Fisher test mean for the significant results in the gender
analysis? Does the result just mean that at least one of those is a true
non-null effect? Is this interesting?
-It wasn’t clear to me why you didn’t report, in applications 2 and 3,
the percent of non-significant results that your analyses suggest are false
negatives, as you did in application 1. In application 3, I’m guessing
this is because the confidence interval around this point estimate is so
wide that the point estimate would be misleading – is this correct? Is
this also the explanation for why you didn’t report this statistic in
application 2?

I’ll end with one last general point that is more a reflection than a
suggestion for you. Please feel free to ignore it completely – I don’t
like it when editors act like they are a co-author on the paper, and this is
definitely a case where my disagreement is a matter of opinion rather than
fact, so you have no obligation whatsoever to take my advice. Instead,
consider it a sample of N = 1 of how some readers may interpret your
argument.
I found myself disagreeing with your overall conclusion that the emphasis on
false positives is “unwarranted” (abstract, p. 3). In my view, your
data do not show that false negatives are a big problem, because your data
do not speak to how non-significant results are interpreted in the papers
themselves, nor what impact they have on subsequent research (i.e., do they
deter others from pursuing the same question?). Many of the arguments that
we should pay more attention to false negatives rest on these assumptions.
Moreover, as you state yourself, the vast majority of focal tests in
published psychology papers are statistically significant, which suggests
that the negative results are likely non-focal tests. This puts more burden
on those who claim we should be paying more attention to false negatives to
demonstrate that these negative results actually have an impact on research
decisions. Moreover, it raises the possibility that researchers are not
“neglecting effects due to a lack of statistical significance”
(abstract), but neglecting them because they don’t care about them. If
they did care about those effects, they may very well have invested more
effort into getting them to reach the threshold for significance (either
through increasing sample size, reducing measurement error, or p-hacking).

In summary, the reviewer and I found much to like about your paper, and also
had suggestions for improving your manuscript. I look forward to receiving
your revision.

To access your submission account, follow the below instructions:
1) login to the journal webpage with username and password
2) click on the submission title
3) click 'Review' menu option
4) download Reviewed file and make revisions based on review feedback
5) upload the edited file
6) Click the 'notify editor' icon and email the confirmation of
re-submission and any relevant comments to the journal.

Please ensure that your revised files adhere to our author guidelines, and
that the files are fully copyedited/proofed prior to upload. Please also
ensure that all copyright permissions have been obtained. This is the last
opportunity for major editing, therefore please fully check your file prior
to re-submission.

If you have any questions or difficulties during this process, please do
contact us.

Please could you have the revisions submitted by January 4th. If you cannot
make this deadline, that's no problem, just let us know as early as
possible.

Kind regards,

Simine Vazire
University of California, Davis, USA
simine@gmail.com
------------------------------------------------------
Reviewer B:


1) General comments and summary of recommendation
Describe your overall impressions and your recommendation, including changes
or revisions. Please note that you should pay attention to scientific,
methodological, and ethical soundness only, not novelty, topicality, or
scope. A checklist of things to you may want to consider is below:
 - Are the methodologies used appropriate?
 - Are any methodological weaknesses addressed?
 - Is all statistical analysis sound?
 - Does the conclusion (if present) reflect the argument, is it supported
by data/facts?
 - Is the article logically structured, succinct, and does the argument
flow coherently?
 - Are the references adequate and appropriate?:
This article attempts to estimate the proportion of False Negative (FN)
results in the published psychological literature. Overall, this is a well
written article.
A few minor comments/thoughts about the first analysis (since the methods in
the other two analyses were similar/the same). In the first analysis, the
authors examined a very large number of non-significant results from eight
psychology journals. The results were automatically (using an R package
statcheck) extracted from over 14 thousand articles. There were multiple
results extracted per paper; in fact, it seems that on average 3.5
non-significant results per paper were extracted (Table 3). The assumption
of independence required for the application of the Fisher test (which
assumes the p-values values are uniformly distributed) is therefore
potentially violated. The authors deal with this by computing the ICC for
non-significant results, and report it to be .001, which they suggest
indicates independence of p-values within a paper. I would like to see a bit
more discussion here as to whether this test is sufficient to dismiss the
violation of the independence assumption as non-consequential in most/many
of these articles. Are there are other references that have applied the ICC
to p-values? I’m unfamiliar with this application. For instance, data on
which ICC is computed are typically normally distributed. From a more
practical standpoint, if one looked at a few articles at random, does the
assumption that the same data are not used for the non-focal test seem
reasonable? If an average number of studies per paper is 3, and each study
reports a gender analysis that comes out non-significant, I believe in the
independence of the resulting p-values. But I can imagine in many articles
other scenarios are at play and this is not the case. What would be the
impact on the analyses?
The argument is that non-significant results reported in papers tend to be
non-focal, and therefore are not p-hacked. The focal results in each paper,
however, are either p-hacked or otherwise selected for significance
(publication bias). This may be a bit far fetched but I wondered about
whether selecting on main results can affect the distribution of the
non-focal results in some way. It may be an interesting thought exercise.
Related, is there any way in which publication bias/p-hacking can select for
non-significant non-focal results? For instance, perhaps some journals are
more likely to publish a “clean” set of results, where the main effects
are found but the interactions with gender etc. are not significant.
I need a clarification on the meaning of the k=1 line in Table 4. I can only
understand the test if more than one result per paper exists. Possibly I
missed something. If the results for k=1 are indeed meaningful, can these
values be related to an estimate of power to detect an average non-focal
result? (i.e., use a different denominator to get the percentage). It may
help the reader to place them in context. A comparison between average power
for non-focal effects and what we know about average power for focal effects
(e.g., Cohen) could be informative. A priori, as we expect many non-focal
effects to be small or zero (so I’m using the word “power” here more
loosely to refer to rates of non-rejection of H0), this number should be way
lower than the average power for focal effects, which is already dangerously
low in psychology, as we know.
The later rows in Table 4, as well as the average row, are a bit misleading
because it is unfair to expect a statistical test to perform
perfectly—when k>10, is it really reasonable to expect that not a single
non-significant p-value in any article would not be a false negative? It
makes it look like the problem with FNs is very bad (e.g., for JPSP, the
rate is 94% with k>20). I would caution the reader against a pessimistic
interpretation of the numbers in the high k rows, or in the average row
(which is almost too misleading to be computed).

One could also ask, somewhat cynically, whether we really learn anything
from a paper that says that, on average when we examine false negative
results reported in a paper (with the average number of them being 3.5),
that at least one of them is wrong. This seems like something we know a
priori. What would be your response to this? Disclaimer: I do find the paper
worthwhile; I just wonder if the first analysis is over-stated.



2) Figures/tables/data availability:
Please comment on the author’s use of tables, charts, figures, ifrelevant.
Please acknowledge that adequate underlying data is available to ensure
reproducibility (see open data policies per discipline of Collabra here).:
Yes



3) Ethical approval:
If humans or animals have been used as research subjects, and/or tissue or
field sampling, are the necessary statements of ethical approval by a
relevant authority present? Where humans have participated in research,
informed consent should also be declared.
If not, please detail where you think a further ethics
approval/statement/follow-up is required.:
Human ethics not relevant.



4) Language:
Is the text well written and jargon free? Please comment on the quality of
English and any need for improvement beyond the scope of this process.:
The article is well written. I would not call it "jargon free", and it
could be written more plainly. I'm not the best person to comment on this.

------------------------------------------------------
________________________________________________________________________
Collabra
http://www.collabra.org/
@collabraoa


<!DOCTYPE html>
<html id="_htmlTag" lang="en">

<head><meta charset="utf-8" /><title>
	Visualizing minimal ingroup and outgroup faces: Implications for impression...: EBSCOhost
</title>
	
<link rel="icon" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/ehost/favicon.ico" type="image/x-icon" />
<link rel="shortcut icon" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/ehost/favicon.ico" type="image/x-icon" />

	<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/ehost/master_bundle.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/rtac.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/common/abody.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/texttospeech/readspeaker.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/selecteddatabasescontrol.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/page/detail.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/carousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/bookcarousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/emailprintdialog.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/print.css" media="Print" />
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/ie8.css" media="All" /><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/ie7.css" media="All" /><![endif]-->
<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/css/_layout2/ie6.css" media="All" /><![endif]-->
<!--##EPCSS##-->
	
	<script>
var ep = {"version":"14.3.0.380","baseImagePath":"http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/","brandingPath":"http://imageserver.ebscohost.com/branding/","interfaceId":"ehost","cssLayout":2,"messages":{"Close":"Close","Loading":"Loading","show_this_area":"Show this area","hide_this_area":"Hide this area","column1-closed":"Show Left Column","column1-open":"Hide Left Column","column2-closed":"Show Right Column","column2-open":"Hide Right Column","sh_more":"Show More","sh_less":"Show Less","field_required":"This field is required.","email_invalid_error":"Please provide a valid email address.","your_subject_may_not_contain_html_markup":"Your subject may not contain HTML markup.","your_comments_may_not_contain_html_markup":"Your comments may not contain HTML markup.","err_sending_email":"Error Sending Email","your_message_may_not_contain_html_markup":"Your message may not contain HTML markup."},"clientData":{"currentRecord":{"Db":"pdh","Tag":"AN","Term":"2014-19904-003"},"rtacView":"detail","rtacTimeout":30,"addThis":{"widgetUrl":"http://s7.addthis.com/js/250/addthis_widget.js#username=ebscohost","bookmarkUrl":"http://www.addthis.com/bookmark.php?v=250\u0026username=ebscohost"},"hoverPreviewLabelData":"{\"Abstract\":\"Abstract\",\"Date\":\"Date\",\"Source\":\"Source\",\"Subjects\":\"Subjects\",\"Title\":\"Title\",\"Citation\":\"Detail\",\"FullCitation\":\"Detailed Record\",\"AddToFolder\":\"Add to folder\",\"RemoveFromFolder\":\"Remove from folder\",\"FolderItem\":\"Folder Item\",\"AddExternalRecToFolder\":\"Add citation to Other Contents Folder\",\"RemoveExternalRecFromFolder\":\"Remove citation from Other Content Sources Folder\",\"AddToFolderTitle\":\"Add result to folder\",\"RemoveFromFolderTitle\":\"Remove result from folder\",\"AddRemoveToFolder\":\"Add/Remove \",\"AddRemoveToFolderTitle\":\"Add or remove from folders\",\"PublicationType\":\"Publication Type\",\"Database\":\"Database\",\"Duration\":\"Length (hours:minutes)\"}","plink":"http://search.ebscohost.com/login.aspx?direct=true\u0026db=pdh\u0026AN=2014-19904-003\u0026site=ehost-live"},"templates":{},"pageScripts":["bundled/jqueryplusui.js","bundled/prototype.js","bundled/underscore.js","bundled/_layout2/master.js","bundled/ehost/page/detail.js","ep/selectdb.js","ep/widgets/epeditor.js","ckeditor/ckeditor.js","ckeditor/adapters/jquery.js","bundled/notesmodal.js","jquery/plugins/jquery.ba-bbq.js","ep/controller/realtimeavailabilitycontroller.js","ep/jqueryplugins/scrollto.js","bundled/texttospeech.js","ep/common/menubar.js"],"relativeRequestPath":"detail/detail","sid":"9c5aba2c-c38b-4471-a994-544fbf07c11d@sessionmgr111","vid":0,"existingReturnUrl":"","newReturnUrl":"/ehost/detail/detail?sid=9c5aba2c-c38b-4471-a994-544fbf07c11d@sessionmgr111\u0026vid=0\u0026hid=113\u0026bdata=JnNpdGU9ZWhvc3QtbGl2ZQ=="}
</script>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/javascript/bundled/ep_boot.js"></script>
<!--[if lt IE 9]>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/javascript/html5shiv/html5.js"></script>
<![endif]-->
<script>

ep.boot(function() {
	ep.updateSearchMode();
	focusOnMainContent();
	
ep.util.url.updateHash("db=pdh&AN=2014-19904-003");

	ep.getInstance( { epId: 'ep.controller.page.CitationController' });
	ep.getScreenResolution();

},
null);
</script>
<!--##EPJS##-->
	
	
</head>
<body id="ctl00_ctl00__bodyTag" class="column1-open column2-open limited-scope no-skin detail ehost">
	
	

	<div id="epAjaxActive">Loading...</div>	
	<form method="post" action="detail?sid=9c5aba2c-c38b-4471-a994-544fbf07c11d%40sessionmgr111&amp;vid=0&amp;hid=113&amp;bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d" onsubmit="javascript:return WebForm_OnSubmit();" id="aspnetForm">
<div class="aspNetHidden">
<input type="hidden" name="AddToFolderClientIDs" id="AddToFolderClientIDs" value="" />
<input type="hidden" name="RelRequestPath" id="RelRequestPath" value="detail/detail" />
<input type="hidden" name="__EVENTTARGET" id="__EVENTTARGET" value="" />
<input type="hidden" name="__EVENTARGUMENT" id="__EVENTARGUMENT" value="" />
<input type="hidden" name="__sid" id="__sid" value="9c5aba2c-c38b-4471-a994-544fbf07c11d@sessionmgr111" />
<input type="hidden" name="__vid" id="__vid" value="0" />
<input type="hidden" name="__CUSTOMVIEWSTATE" id="__CUSTOMVIEWSTATE" value="H4sIAAAAAAAEAI1W728UNxAVl9v8AJVUgPIxGVSpSqS7VXIJakJRpRCIGrVARBBfkW/t3bXw2lfbm3D8Vf0P22d7L7kLacWXO9trz3sz82bsf+6t8/Vs7eBodDg6/OVof32jt/n4I1OSMy/ei79a4fwbw0Xv3kavXOfdT+9e2NdXTFdZT2gsLIWFrFDMuexFYVTb6L2hmQhNaTJKEyUb6QUfugJT0mboPktNXHgmFYnaOA9bGSB+nOEEuysfpZNjJWrOeytxPcBtLdUdE4x4b3W9G/HeWneWb/TLm9X+zfB+HFXJm9UEtRyQHnx6euZFc2Ja7XtLG8vRSAfSyx6VxjbMD+FCQ6zw8lLwXu8arAdy6xv3N1dPnDuJkXhsRWEsH/opvK19o4al33xwYpqGaf6WNSI7en2e/ykqVkzzE6O9NUoJ6/JXMSL5acRz+Wmr1Afxxaf5y9Z7ozfXOzvHtmoboT3f7H96evGyFzyDj0s3QUi8trLs0TyfCS9BZ2s5e/FdHM55eReNrRW+tTrD/AE4/c1+2JMtb+/vH/yxM0emDFHsByXNhxVp6Mc8L2+tZT+eBHnQe1EKK3Qh3NZKdn9u1hlLm7PtvdHuzl075lKdzYbA63UHn5xKzekCWlTMAsy1yrvNlQ/GqA9ykj15L7yV4lIQU4ps+gyYvx9Chy1q46vUFTVSy4YpwtiadkLIBJnWp0nJwOU5nTUTJQvmpdGOIB6SzQT2XJgPiHkvfctFGOLwWNTsUhqb0xtjBfmaaTrYpalg1pEpAxEMi5qwTZCrzZUOmzxNhJkoAcxLIDSiGYt0wNdC2mt+Us8WvoODy+l3RHRAV9CJNRPjxAIWU84Ehxrisoyx9xQ0GOLRsEpE/P+NUMczwQLFo9EEglRPJwb/TrqEOANAQDsfwGURqwiqlePWw4yJRq6EUkNYZGMlXQ1FjSVzorPIrISJ0prmmmLihEyJylj5NWYspzNNFwjOlPYGNGHWy0JOmPYO5pEgFLisNGwDcy/4O0pmHLUuCIRRbIeyuIUCS4zLqsnpLeoEAUBmLxEKAT+sFSqCJ9c6C2WnoZCKQvAW4FcswCTwwgoQp8soThjTXFgQiDn4T3ViX5BBiGOUZ04X7diFhq+9mgZWIxxxrAnp1gytbhbbeLoSWthEClEGU0wCm5idqH7CJ0YOVJIavGUyZBOpcwnwQy1m1QWePDgZTbAkhBn1iBf8DeGOsQgMqQlFouRngXGslXBm0UUQFqg/wMbagBjEvPppW+RVPgCz1nmgDSAAG/+l9hCQhJM+lohNzX4np9dfPOIb0utrKLSUcTLodDKKqWRq5saCC7cC3lHD1ujJAsP4JfgomEO2+U2ZJle5nMvmLbupohKf/dgm7mLTaTi6PqMQTS+2MILYopY14ebQpoGaK9xbOaGFojtOZ54f4BJvgO7tTRJhQUKQk3qKcFc4PA01Y7AYtlxJXyf8K2MRTI2sJB6FQruB98GrZpyCKWbWgNAWPpRA6nCEZmiDAGfefevFrQhBeKYSOGoHnV4dXAgqdW1VpTYE9otFy2WQSJEyV4RwJGF90/7uzEanaQimK9LQjkgGxBKCluH8jVhv8gRJSC9dExrf3W37dtfePnfT4uzt6Tt6xdD90PRwv4U7n7aLHRrt7h3Q8fnxIN1ssqpReoGuxUtmh05DsM6FLcQkVvZZR+TdLJrp/pvfcsxRvrUxsL93SFNcPD+jZfxKRqEJ7eD+fZDu+PA2uH7QpZv54eLC9asgnljLfnrxdDgMPc4+J/fscHf32f5R3jCp8/hOJBoOf+P/AhGkGXi/CgAA" />
<input type="hidden" name="__ScreenResolution" id="__ScreenResolution" value="" />
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="" />
</div>

<script type="text/javascript">
//<![CDATA[
var theForm = document.forms['aspnetForm'];
if (!theForm) {
    theForm = document.aspnetForm;
}
function __doPostBack(eventTarget, eventArgument) {
    if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
        theForm.__EVENTTARGET.value = eventTarget;
        theForm.__EVENTARGUMENT.value = eventArgument;
        theForm.submit();
    }
}
//]]>
</script>


<script type="text/javascript">
//<![CDATA[
function WebForm_OnSubmit() {
ClearTarget();
return true;
}
//]]>
</script>

		<!--[if lt IE 7]>	
		

<div class="ie6_req_block">
	<div class="ie6_req_text">
		IE6 users please note our browser requirements are changing! See  <a href="http://support.epnet.com/knowledge_base/detail.php?id=25" target="_blank" title="EBSCO's Support Site">EBSCO's Support Site</a>  for more information.
	</div>
</div>
		<![endif]-->
		
		<div id="outerContainer">
			<div id="innerContainer">
				
					
				
				
	
	
		

				
	
	

				
				<div id="header" class="clearfix" role="banner" >
					<div id="pageInstruction" tabindex="1" class="hidden">citation_instruction</div><p tabindex="0" class="hidden"><a href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&lang=en&feature_id=access&TOC_ID=Always&SI=0&BU=0&GU=1&PS=0&ver=&dbs=pdh')">Accessibility Information and Tips</a> Revised Date: 07/2011</p>
					<h1 title="Visualizing minimal ingroup and outgroup faces: Implications for impressions, attitudes, and behavior" class="hidden">Visualizing minimal ingroup and outgroup faces: Implications for impressions, attitudes, and behavior</h1>
					
					
	<div class="customerLogo"><a href="javascript:__doPostBack('ctl00$ctl00$FindField$customerLogo');"><img src="http://www.tilburguniversity.edu/static/uvtpresentation/images/framework/logo.jpg" alt="Library Logo" /></a></div>
	

					
				</div>
					<div id="mainContentArea">
						<div id="content" role="main" class="text-normal" >
							
	
	<div class="content-header" >
	 

	</div>
	
	
	

	
	<div id="ToolPanelContent" class="bg-p2" >
		<div class="wrapper clearfix">
		</div>
		<a  href="#" title="Close Panel" class="close-panel"></a>
	</div>

	<!-- If citation is being displayed it will be rendered inside this placeholder.
		 If citation is not being displayed, full text will be rendered in this placeholder. -->
	<div class="citation-wrapping-div"><h2 class="hidden" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions">Detailed Record</h2><dl id="citationFields" class="citation-fields" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><dt class="title-label">Title:</dt><dd class="citation-title color-s4"><a name="citation"><span>Visualizing minimal ingroup and outgroup faces: Implications for impressions, attitudes, and behavior.</span></a></dd><dt>Authors:</dt><dd>Ratner, Kyle G., Department of Psychology, The Ohio State University, Columbus, OH, US, <a data-auto="ep_link_" href="mailto:kyle.ratner@gmail.com" id="linkkyle.ratner@gmail.com" title="kyle.ratner@gmail.com" data-title="kyle.ratner@gmail.com">kyle.ratner@gmail.com</a> <br />Dotsch, Ron, Behavioural Science Institute, Radboud University Nijmegen, Netherlands<br />Wigboldus, Daniel H. J., Behavioural Science Institute, Radboud University Nijmegen, Netherlands<br />van Knippenberg, Ad, Behavioural Science Institute, Radboud University Nijmegen, Netherlands<br />Amodio, David M., Department of Psychology, New York University, NY, US, <a data-auto="ep_link_" href="mailto:david.amodio@nyu.edu" id="linkdavid.amodio@nyu.edu" title="david.amodio@nyu.edu" data-title="david.amodio@nyu.edu">david.amodio@nyu.edu</a> </dd><dt>Address:</dt><dd>Ratner, Kyle G., Department of Psychology, The Ohio State University, 1827 Neil Avenue, Columbus, OH, US, 43210, <a data-auto="ep_link_" href="mailto:kyle.ratner@gmail.com" id="linkkyle.ratner@gmail.com" title="kyle.ratner@gmail.com" data-title="kyle.ratner@gmail.com">kyle.ratner@gmail.com</a> </dd><dt>Source:</dt><dd>Journal of Personality and Social Psychology, Vol 106(6), Jun, 2014. pp. 897-911</dd><dt>Publisher:</dt><dd>US: American Psychological Association</dd><dt>ISSN:</dt><dd>0022-3514 (Print)<br />1939-1315 (Electronic)</dd><dt>Language:</dt><dd>English</dd><dt>Keywords:</dt><dd>face processing, ingroup favoritism, mental representation, minimal group paradigm, reverse correlation, visual bias, impressions, attitudes, behavior</dd><dt>Abstract:</dt><dd> More than 40 years of research have shown that people favor members of their ingroup in their impressions, attitudes, and behaviors. Here, we propose that people also form different mental images of minimal ingroup and outgroup members, and we test the hypothesis that differences in these mental images contribute to the well-established biases that arise from minimal group categorization. In Study 1, participants were assigned to 1 of 2 groups using a classic minimal group paradigm. Next, a reverse correlation image classification procedure was used to create visual renderings of ingroup and outgroup face representations. Subsequently, a 2nd sample naive to the face generation stage rated these faces on a series of trait dimensions. The results indicated that the ingroup face was significantly more likely than the outgroup face to elicit favorable impressions (e.g., trusting, caring, intelligent, attractive). Extending this finding, Study 2 revealed that ingroup face representations elicited more favorable implicitly measured attitudes than did outgroup representations, and Study 3 showed that ingroup faces were trusted more than outgroup faces during an economic game. Finally, Study 4 demonstrated that facial physiognomy associated with trustworthiness more closely resembled the facial structure of the average ingroup than outgroup face representation. Together, these studies suggest that minimal group distinctions can elicit different mental representations, and that this visual bias is sufficient to elicit ingroup favoritism in impressions, attitudes and behaviors. (PsycINFO Database Record (c) 2014 APA, all rights reserved) (journal abstract)</dd><dt>Subjects:</dt><dd>*Face Perception; *Ingroup Outgroup; *Visual Perception; Attitudes; Behavior; Impression Formation</dd><dt>Classification:</dt><dd>Visual Perception (2323)<br />Group &amp; Interpersonal Processes (3020)</dd><dt>Population:</dt><dd>Human</dd><dt>Location:</dt><dd>US</dd><dt>Age Group:</dt><dd>Adulthood (18 yrs &amp; older)</dd><dt>Tests &amp; Measures:</dt><dd>Numerical Estimation Style Test</dd><dt>Grant Sponsorship:</dt><dd>Sponsor: National Science Foundation<br />Other Details: Graduate Research Fellowship<br />Recipients: No recipient indicated<br /><br />Sponsor: New York University<br />Other Details: Dean’s Dissertation Fellowship<br />Recipients: No recipient indicated<br /><br />Sponsor: Sponsor name not included<br />Other Details: Douglas and Katharine Fryer Thesis Fellowship<br />Recipients: Ratner, Kyle G.<br /></dd><dt>Methodology:</dt><dd>Empirical Study; Quantitative Study</dd><dt>Format Covered:</dt><dd>Electronic</dd><dt>Publication Type:</dt><dd>Journal; Peer Reviewed Journal</dd><dt>Document Type:</dt><dd>Journal Article</dd><dt>Publication History:</dt><dd>Accepted Date: Feb 10, 2014; Revised Date: Feb 7, 2014; First Submitted Date: Aug 27, 2013</dd><dt>Release Date:</dt><dd>20140519</dd><dt>Copyright:</dt><dd> American Psychological Association. 2014.</dd><dt>Digital Object Identifier:</dt><dd>10.1037/a0036498</dd><dt>PsycARTICLES Identifier:</dt><dd>psp-106-6-897</dd><dt>Accession Number:</dt><dd><strong xmlns:Translation="urn:EBSCO-Translation">2014-19904-003</strong></dd><dt>Number of Citations in Source:</dt><dd>120</dd></dl></div>
		

		<div class="widget-loading loading"></div>
	
		<!-- WorldCat Widgets-->
		

	<!-- Full text will be rendered in this placeholder if citation is being displayed with
	full text. -->
	<div class="ft-translation hidden"><label for="transLanguage">Translate Full Text:</label></div><div class="ft-translation"><a name="Translate"> </a><select id="transLanguage" name="transLanguage" title="Choose Language"><option value="" selected="selected">Choose Language</option><option value="Arabic">الإنجليزية/العربية</option><option value="Bulgarian">английски език/български</option><option value="SimplifiedChinese">英语/简体中文</option><option value="TraditionalChinese">英語/繁體中文</option><option value="Czech">angličtina/čeština</option><option value="Danish">Engelsk/dansk</option><option value="Dutch">Engels/Nederlands</option><option value="French">Anglais/Français</option><option value="German">Englisch/Deutsch</option><option value="Greek">Αγγλικά/Ελληνικά</option><option value="Hausa">English/Hausa</option><option value="Hebrew">אנגלית/עברית</option><option value="Hindi">अंग्रेज़ी/हिंदी</option><option value="Hungarian">angol/magyar</option><option value="Indonesian">Inggris/bahasa Indonesia</option><option value="Italian">Inglesi/Italiano</option><option value="Japanese">英語/日本語</option><option value="Korean">영어/한국어</option><option value="Norwegian">Engelsk/Norsk</option><option value="Persian">انگليسی/فارسی</option><option value="Polish">angielski/polski</option><option value="Portuguese">Inglés/Português</option><option value="Pashto">English/Pashto</option><option value="Romanian">Engleză/română</option><option value="Russian">Английский/Русский</option><option value="Spanish">Inglés/Español</option><option value="Serbian">English/Serbian</option><option value="Swedish">Engelska/svenska</option><option value="Thai">อังกฤษ/ไทย</option><option value="Turkish">İngilizce/Türk</option><option value="Ukranian">Англійська/Українська</option><option value="Urdu">انگریزی/اردو</option></select>&nbsp;<input type="button" id="translateBtn" class="translate" value="Translate" title="Translate" /><input type="button" id="translateOriginal" class="translate" value="Back to English" title="Back to English" /></div><div id="translationProgressContainer" style="display: none;"><span>Translation in Progress:</span><div class="translationProgressBar"><div id="translationProgressBar" class="bg-p1"> </div></div></div><div id="translationErrorContainer" class="medium-normal translation-message" style="display: none;"> </div><div id="translationDisclaimerContainer" style="display: none;"><div class="translation-message"><span class="medium-bold"><span class="txt-red" id="translationDisclaimerLine1"> </span></span><span class="medium-normal" id="translationDisclaimerLine2"> </span><span class="medium-bold" id="translationDisclaimerLine3"> </span><div class="medium-normal">Translations powered by Language Weaver Service<br /></div></div></div><script type="text/javascript">
				ep.getInstance("ep.controller.control.translation");
				ep.require( "common/translation.css" );
			</script><div class="full-text-container border" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><h2 class="hidden">HTML Full Text</h2><h2 class="ft-title border color-p4 bar4">Visualizing Minimal Ingroup and Outgroup Faces: Implications for Impressions, Attitudes, and Behavior</h2><div class="html-ft-toc"><h3 class="small-bold" id="toc">Contents</h3><ol><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EGDA" id="hd_psp-106-6-897-EGDA" title="The Contribution of Facial Representations to Intergroup Responses">The Contribution of Facial Representations to Intergroup Responses</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EFDA" id="hd_psp-106-6-897-EFDA" title="Clues That Mere Group Categorization Influences Visual Representations">Clues That Mere Group Categorization Influences Visual Representations</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EEDA" id="hd_psp-106-6-897-EEDA" title="Study 1">Study 1</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-ECEDA" id="hd1_psp-106-6-897-ECEDA" title="Method">Method</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EBEDA" id="hd1_psp-106-6-897-EBEDA" title="Results">Results</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EAEDA" id="hd1_psp-106-6-897-EAEDA" title="Discussion">Discussion</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EDDA" id="hd_psp-106-6-897-EDDA" title="Study 2">Study 2</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-ECDDA" id="hd1_psp-106-6-897-ECDDA" title="Method">Method</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EBDDA" id="hd1_psp-106-6-897-EBDDA" title="Results">Results</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EADDA" id="hd1_psp-106-6-897-EADDA" title="Discussion">Discussion</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-ECDA" id="hd_psp-106-6-897-ECDA" title="Study 3">Study 3</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-ECCDA" id="hd1_psp-106-6-897-ECCDA" title="Method">Method</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EBCDA" id="hd1_psp-106-6-897-EBCDA" title="Results">Results</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EACDA" id="hd1_psp-106-6-897-EACDA" title="Discussion">Discussion</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EBDA" id="hd_psp-106-6-897-EBDA" title="Study 4">Study 4</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-ECBDA" id="hd1_psp-106-6-897-ECBDA" title="Method">Method</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EBBDA" id="hd1_psp-106-6-897-EBBDA" title="Results">Results</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EABDA" id="hd1_psp-106-6-897-EABDA" title="Discussion">Discussion</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-EADA" id="hd_psp-106-6-897-EADA" title="General Discussion">General Discussion</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EDADA" id="hd1_psp-106-6-897-EDADA" title="Inferring a Causal Influence of Facial Representations on Intergroup Responses">Inferring a Causal Influence of Facial Representations on Intergroup Responses</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-ECADA" id="hd1_psp-106-6-897-ECADA" title="Implications for Mental Simulations Involving Ingroup and Outgroup Members">Implications for Mental Simulations Involving Ingroup and Outgroup Members</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EBADA" id="hd1_psp-106-6-897-EBADA" title="Implications for Intergroup Face Perception and Self-Regulation">Implications for Intergroup Face Perception and Self-Regulation</a></li><li class="link-medium html-toc-hd1"><a data-auto="ep_link_" href="#psp-106-6-897-EAADA" id="hd1_psp-106-6-897-EAADA" title="Conclusion">Conclusion</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-R" id="hd_psp-106-6-897-R" title="Footnotes">Footnotes</a></li><li class="link-medium"><a data-auto="ep_link_" href="#psp-106-6-897-E0YD0ACA" id="hd_psp-106-6-897-E0YD0ACA" title="References">References</a></li></ol></div><section id="TextToSpeech" class="full-text-content textToSpeechDataContainer" data-text-to-speech-cache-key="pdh_2014-19904-003" data-text-to-speech-title="Visualizing minimal ingroup and outgroup faces: Implications for impressions, attitudes, and behavior." data-text-to-speech-author="Ratner, Kyle G." data-text-to-speech-additional-filename="20140601"><span id="textToSpeechPlaceholder"> </span><div class="center" xmlns:Translation="urn:EBSCO-Translation"><strong>By: Kyle G. Ratner</strong><br /><em>Department of Psychology, The Ohio State University, and Department of Psychology, New York University</em>;
<br /><strong>Ron Dotsch</strong><br /><em>Behavioural Science Institute, Radboud University Nijmegen</em><br /><strong>Daniel H. J. Wigboldus</strong><br /><em>Behavioural Science Institute, Radboud University Nijmegen</em><br /><strong>Ad van Knippenberg</strong><br /><em>Behavioural Science Institute, Radboud University Nijmegen</em><br /><strong>David M. Amodio</strong><br /><em>Department of Psychology, New York University</em>;</div><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Acknowledgement: </strong>This research was supported by a National Science Foundation Graduate Research Fellowship, a New York University Dean’s Dissertation Fellowship, and a Douglas and Katharine Fryer Thesis Fellowship to Kyle G. Ratner. This research was conducted as part of Kyle G. Ratner’s doctoral dissertation at New York University under the sponsorship of David M. Amodio. We thank dissertation committee members Susan Andersen, Yaacov Trope, and Jay Van Bavel for their feedback during various stages of this project and Lisa Kaggen, Amy Krosch, and Petra Schmid for their comments on earlier drafts of this paper. A special thanks goes to Dustin Chien, Alexis Donovan, Manasa Kanthamneni, Tim Pasternak, Julia Schmidt, Bryan Tay, Danielle Troumouliaris, and Shelley Yang for their assistance with data collection.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Note: </strong>Kimberly Quinn served as the action editor for this article.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Intergroup biases are not always the result of long-standing rivalries. They may arise even in conditions where people do not know each other and have no conflicts of interest or any preexisting animosity toward each other. Indeed, mere identification with one of two distinct groups is sufficient to elicit a preference for one group over another (
        Tajfel, 1970; 
        Tajfel, Billig, Bundy, &amp; Flament, 1971; 
        Tajfel &amp; Turner, 1986). Once formed, a 
        <em>minimal group</em> identity can influence a wide range of responses, including impressions, attitudes, and behaviors that have consequences for members of the ingroup as well as the outgroup (e.g., 
        Ashburn-Nardo, Voils, &amp; Monteith, 2001; 
        Brewer &amp; Silver, 1978; 
        Locksley, Ortiz, &amp; Hepburn, 1980; 
        Otten &amp; Wentura, 1999; 
        Tajfel et al., 1971; 
        Van Bavel &amp; Cunningham, 2009).
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In the last decade, researchers have begun to examine minimal group effects on visual perception and memory (e.g., 
        Bernstein, Young, &amp; Hugenberg, 2007; 
        Hugenberg &amp; Corneille, 2009; 
        Ratner &amp; Amodio, 2013; 
        Van Bavel, Packer, &amp; Cunningham, 2008, 
        2011). The recent focus on this topic reflects a broader recognition in social psychology that studying how people make sense of faces and other visual information provides insight into the nature of social interactions (
        Hassin &amp; Trope, 2000; 
        Macrae, Quinn, Mason, &amp; Quadflieg, 2005; 
        Todorov, Said, Engell, &amp; Oosterhof, 2008; 
        Zebrowitz &amp; Montepare, 2008). However, unlike other minimal group research that has focused on idealized representations of ingroup members as pleasant and trustworthy (e.g., 
        Brewer &amp; Silver, 1978; 
        Tajfel et al., 1971), the work in visual processing has largely focused on how group membership may affect the degree of attention and perceptual resources allocated to the processing of ingroup as opposed to outgroup faces (e.g., 
        Bernstein et al., 2007; 
        Ratner &amp; Amodio, 2013; 
        Van Bavel et al., 2008, 
        2011; 
        Young &amp; Hugenberg, 2010). As a result, the literature has not addressed the question of how one’s actual mental representation of an ingroup or outgroup member’s face may be distorted by the top-down influence of mere group categorization or how such distortion might contribute to intergroup bias.
      </p><a id="psp-106-6-897-EGDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_8" title="The Contribution of Facial Representations to Intergroup Responses">The Contribution of Facial Representations to Intergroup Responses</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Facial representations are key to social interactions because they convey information about a person’s intentions, reactions, and dispositions. Not surprisingly, people are highly attuned to facial information. Among both children and adults, faces receive preferential attention and processing over other objects (
        Birmingham, Bischof, &amp; Kingstone, 2009a, 
        2009b; 
        Goren, Sarty, &amp; Wu, 1975; 
        Johnson &amp; Morton, 1991), and this preference is supported by the rapid encoding of faces in neural regions involved in high-level vision (
        Bentin, Allison, Puce, Perez, &amp; McCarthy, 1996; 
        Haxby, Hoffman, &amp; Gobbini, 2000; 
        Kanwisher, McDermott, &amp; Chun, 1997). Neuroimaging findings suggest that mental images of faces are processed in much the same way as the direct perception of a face. Indeed, the same neural regions involved in direct face perception are also activated in response to an imagined face, suggesting that, as with perceiving actual faces, mental imagery of a face may profoundly shape the way information about a person is processed (
        Ishai, Haxby, &amp; Ungerleider, 2002; 
        Mechelli, Price, Friston, &amp; Ishai, 2004; 
        O’Craven &amp; Kanwisher, 2000).
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Given the weight afforded to facial cues during information processing, it is not surprising that information about appearance figures prominently in group-based knowledge structures. 
        Kessler and McKenna (1978) reported that when people are asked to make gender inferences, they rely on physical characteristics to a larger degree than traits and behaviors. Furthermore, the inclusion of information about physical attributes increases the vividness and specificity of trait-based stereotypes (
        Andersen &amp; Klatzky, 1987) and the likelihood that trait stereotypes will be applied during impression formation (
        Deaux &amp; Lewis, 1984). Thus, representations of physical appearance promote the use of group-based stereotypes when making trait inferences.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The visual component of social group representations not only triggers trait inferences but also contributes to attitudes and behaviors. For instance, in the United States, Afrocentric features have been shown to activate negative attitudes toward Black individuals (
        Livingston &amp; Brewer, 2002). In other work, White Americans who simulated the role of a patrolling police officer during a video game scenario were more likely to shoot unarmed Black men whose appearance was more prototypical of the Black stereotype (
        Ma &amp; Correll, 2011). Furthermore, it has been shown that Black men accused of a crime receive harsher sentences when their appearance is closer to this stereotypical prototype (
        Blair, Judd, &amp; Chapleau, 2004; 
        Eberhardt, Davies, Purdie-Vaughns, &amp; Johnson, 2006). Thus, visual representations constitute a powerful mechanism for conveying group-based prejudices.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Group identity is also related to the mental images people form of others, and these images have implications for social judgments and behavior. For example, 
        Beaupré and Hess (2003) found that people were more likely to assume that targets portrayed in neutrally written vignettes were smiling if they shared the same ethnic group as the perceiver. Additionally, 
        Dotsch, Wigboldus, Langner, and van Knippenberg (2008) observed an association between their Dutch participants’ implicitly measured attitudes and the extent to which these participants formed mental images of Moroccan faces as appearing criminal and untrustworthy. In other work, 
        Blair, Ma, and Lenton (2001) found that the strength of people’s stereotypic associations could be weakened by asking them to engage in counterstereotypical mental imagery. Similarly, imagined contact with outgroups has been shown to reduce intergroup anxiety and improve attitudes toward several groups, including the elderly, homosexuals, and Muslims (
        Crisp &amp; Turner, 2009; 
        Turner &amp; Crisp, 2010; 
        Turner, Crisp, &amp; Lambert, 2007). Together, the existing literature suggests that mental representations are important for guiding intergroup responses. However, the present question—of whether the simple act of categorizing others into the same or different social group as our own contributes to these mental representations and subsequent reactions—has not yet been directly addressed.
      </p><a id="psp-106-6-897-EFDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_14" title="Clues That Mere Group Categorization Influences Visual Representations">Clues That Mere Group Categorization Influences Visual Representations</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Although most research on visual processing has focused on ingroup processing advantages (e.g., promoting perceptual accuracy), there are clues in the literature that ingroup favoritism under minimal group conditions can arise from distortions in facial representations, as we hypothesize. In the first wave of minimal group studies, 
        Doise and colleagues (1972) examined whether anticipating a cooperative or a competitive context influences images of minimally generated groups and whether this effect on imagery could justify one’s intended behavior toward the outgroup. In their study, they included evaluations of the groups on four physical trait dimensions: blond–dark, tall–short, fat–thin, colorful–quiet. Of importance for the present purposes, they also included a control group that consisted of minimal ingroups and outgroups with no anticipated interaction. Doise and colleagues did not conduct statistical analyses on differences between the ingroup and outgroup representations for the control group, but the data suggest that the ingroup was viewed to have physical traits deemed more favorable than the outgroup. A later study by 
        Johnson (1981) investigated whether people have more favorable mental images of people belonging to their political party (a group with no clear perceptual basis). He found that when participants were presented with photographs of people who could be members either of their political party or of an opposing party, they selected more attractive people as belonging to their party. Most recently, 
        Dunham (2011) reported that rapidly presented happy faces were more likely to be remembered as belonging to ingroup members. This link between happy expressions and the ingroup is consistent with the existence of a top-down expectation that ingroup faces comprise more favorable physiognomic information than outgroup faces.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">On the basis of past work, we propose that mere membership in a social group has implications for the way faces of ingroup and outgroup members are represented in the mind and that differences in these mental representations can influence trait impressions, attitudes, and behaviors toward ingroup and outgroup members. In this way, mental representations of ingroup and outgroup members’ faces may provide a mechanism through which mere group categorization contributes to stereotyping, prejudice, and discrimination. We tested this hypothesis across four studies.</p><a id="psp-106-6-897-EEDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_18" title="Study 1">Study 1</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The objective of Study 1 was to test for differences in participants’ mental representations of ingroup and outgroup faces and to assess whether these differences contribute to trait impressions formed by perceivers. This study was conducted in two parts, in which we (a) obtained visual renderings of participants’ mental representations of minimally defined ingroup or outgroup members and (b) collected trait ratings of these renderings from a separate group of participants naive to the origin of the face images. This approach is described in more detail in what follows.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In Part 1, individuals were assigned to minimal groups and then categorized faces as belonging to either their ingroup or an outgroup. To capture the representational bias that we hypothesize can give rise to ingroup favoritism, we used a technique called 
        <em>reverse correlation image classification.</em> Reverse correlation image classification belongs to a class of techniques that follow 
        <em>reverse correlation</em> logic. Reverse correlation methods examine responses to many different stimuli and infer patterns in the stimuli that may have caused the responses. These patterns can be visualized and provide an approximation of the mental representations upon which participants based their responses. These techniques were developed over 40 years ago and have been applied to study a diverse range of topics, including auditory cognition (
        Ahumada &amp; Lovell, 1971), neurophysiology (
        Ringach &amp; Shapley, 2004; 
        Victor, 2005), and low-level vision (
        Ahumada, 2002; 
        Solomon, 2002). Recently, they have proved useful for investigating representations underlying face categorization (
        Dotsch &amp; Todorov, 2012; 
        Gosselin &amp; Schyns, 2003; 
        Mangini &amp; Biederman, 2004; 
        Martin-Malivel, Mangini, Fagot, &amp; Biederman, 2006) and social cognitive biases in face processing (
        Dotsch et al., 2008; 
        Dotsch, Wigboldus, &amp; van Knippenberg, 2011; 
        Imhoff, Dotsch, Bianchi, Banse, &amp; Wigboldus, 2011; 
        Jack, Caldara, &amp; Schyns, 2012; 
        Karremans, Dotsch, &amp; Corneille, 2011; 
        Young, Ratner, &amp; Fazio, 2014). Thus, reverse correlation image classification provides a purely data-driven method for creating visual renderings of people’s mental representations of faces that are resistant to experimenter bias (
        Mangini &amp; Biederman, 2004; 
        Todorov, Dotsch, Wigboldus, &amp; Said, 2011).
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In Part 2, we assessed whether these visual renderings could reveal biases that contribute to differential impressions of ingroup and outgroup members. To this end, we averaged together the visual renderings (also called classification images) generated for each participant to create grand-averaged ingroup and outgroup classification images. Then, an independent sample of participants was used to assess the impressions elicited by these grand-averaged ingroup and outgroup face representations. In order to determine the types of impressions people might discern from facial representations, we focused on trait dimensions that 
        Oosterhof and Todorov (2008) have shown to be the most common types of impressions that people naturally draw from faces.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Study 1 was designed to test the role of facial representations as a mechanism involved in the effect of minimal group assignment on ingroup favoritism, in a manner analogous to a mediation pattern. Because much previous research has established the “direct path” between minimal group assignment and ingroup favoritism (e.g., 
        Brewer &amp; Silver, 1978), our design adopted the “experimental mediation” approach to examine a mechanism that contributes to this well-established effect (
        Spencer, Zanna, &amp; Fong, 2005). We predicted, in line with the previously reviewed research and theory, that the ingroup facial representation would be rated more highly on traits signaling prosociality (e.g., trustworthiness, caring, sociality) and overall group fitness (e.g., intelligence, attractiveness, confidence) compared with the outgroup face representation.
      </p><a id="psp-106-6-897-ECEDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Part 1: Generating visual renderings of group-based facial representations</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Participants</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">One hundred and seventy-six New York University students were recruited to participate in exchange for course credit. Up to four participants were run simultaneously in separate cubicle rooms.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Procedure</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Upon arrival at the study session, participants were asked to complete a consent form and were told that they would perform several tasks on a computer. Next, a classic “dot estimation” minimal group procedure was used to assign participants to arbitrary, but believable, groups (
              Brown, Collins, &amp; Schmidt, 1988; 
              Gerard &amp; Hoyt, 1974; 
              Mussweiler, Gabriel, &amp; Bodenhausen, 2000; 
              Tajfel et al., 1971).
            </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Numerical Estimation Style Test (NEST)</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants were led to believe that people vary in numerical estimation style, which was defined as the tendency to overestimate or underestimate the number of objects they encounter. To underscore the arbitrary nature of numerical estimation style, we told participants that approximately half the population are overestimators and half are underestimators, and that research has not related numerical estimation style to any other cognitive tendency or personality trait.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants were told they would categorize photographs of students from a previous semester whose numerical estimation style had been determined with a well-established task called the 
                <em>Numerical Estimation Style Test.</em> They were then informed that past research had shown that people are able to reliably detect numerical estimation style from faces and that the purpose of the current study was to test whether people can determine numerical estimation style even when face images appear blurry. The instructions explained that the study was important because often people need to make judgments about others from far distances or at night, when perceptual information is not completely clear. It was crucial that participants accepted this aspect of the cover story, because the reverse correlation procedure requires that visually noisy images are used during the face categorization task.
              </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Next, participants were told that in order to gain a concrete understanding of numerical estimation style and how it is measured, they should first complete the NEST themselves. In brief, the task consisted of estimating the number of dots in 10 rapidly presented dot patterns. At the end of the test, the computer program provided predetermined feedback (counterbalanced across participants), indicating that each participant was either an overestimator or an underestimator. The NEST was not actually used to assess any perceptual tendency; it simply provided a rationale for the manipulation of group assignment.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Several procedures were used to ensure that the novel group category was salient in participants’ minds throughout the entire face categorization task. First, participants were prompted to report their numerical estimation style to the experimenter, which served as a public act of commitment to the group. The experimenter then wrote each participant’s identification number and numerical estimation style in large letters on the cover page of a post-task questionnaire packet and placed the packet in the participants’ line of sight to remind them of their group membership during the categorization task. Participants also typed their numerical estimation style group into the computer, in another behavioral act of association. These procedures served to keep participants’ minimal group membership salient during the critical categorization task.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Face categorization</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Following the NEST, participants completed a forced-choice face categorization task for 450 trials. On each trial, participants viewed two adjacent face images, each consisting of 211 × 270 grayscale pixels. Participants were told that on each trial one of the faces was an overestimator and the other was an underestimator, and that all the people whose faces they were viewing had completed the numerical estimation style test during a previous semester. According to a counterbalanced schedule, some participants were asked on every trial to choose which of the two faces was an overestimator and the other participants were asked on every trial to select the underestimator face. Thus, targets were ingroup members when their numerical estimation style was shared with the participant. Conversely, targets were outgroup members when their numerical estimation style was different from that of the participant.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Each face was actually derived from the same 
                <em>base face</em> image: the grayscale neutral male average face of the Averaged Karolinska Directed Emotional Faces Database (
                Lundqvist &amp; Litton, 1998). Noise patterns were layered on the images, to make each face look unique, distorting the various facial features and overall facial structure. The noise pattern added to each image consisted of 4,092 superimposed truncated sinusoid patches spanning two cycles in six orientations (0°, 30°, 60°, 90°, 120°, and 150°) × five spatial frequencies (1, 2, 4, 8, and 16 patches per image) × two phases (0, π/2), with random contrasts (amplitudes) as parameters.
              </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">On each trial, a random noise pattern was generated. This noise pattern was applied to one of the images in the pair, and the inverse of the noise pattern was added to the other image (see 
                Figure 1). The image with the inverse noise was equally presented on the left and right sides of the screen in a random order. The same noise patterns were used for all participants.
                
<br /><br /><a id="fig1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/781c9c1a5bcf9e529787d37cb5b7fac1/5422c463/pdh/psp/psp-106-6-897-fig1a.gif" alt="psp-106-6-897-fig1a.gif" title="Figure 1. Example stimuli used in the Study 1 face categorization task. (A) Base face, (B) random noise pattern, (C, left) noise pattern superimposed on the face, and (C, right) inverse of the noise pattern superimposed on the base image." /><em>Figure 1. Example stimuli used in the Study 1 face categorization task. (A) Base face, (B) random noise pattern, (C, left) noise pattern superimposed on the face, and (C, right) inverse of the noise pattern superimposed on the base image.</em></span></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">It is important to note that because a response was required on each trial and participants were tasked with selecting on the basis of only one group (either overestimators or underestimators) for all the trials, an equal number of trials were used to generate visualizations of ingroup and outgroup representations (i.e., effects were not biased by the well-established tendency to exclude ambiguous individuals from the ingroup; 
                Castano, Yzerbyt, Bourguignon, &amp; Seron, 2002; 
                Krosch, Berntsen, Amodio, Jost, &amp; Van Bavel, 2013; 
                Leyens &amp; Yzerbyt, 1992; 
                Quanty, Keats, &amp; Harkins, 1975).
              </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Face representation data processing</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The logic of the reverse correlation method is that, on each trial, participants solve the task by comparing the two faces presented on the screen with their mental representation of ingroup or outgroup members, as defined by numerical estimation style. They presumably select whichever face best matches this mental representation. In order to create clear visualizations of social category representations, several hundred of these forced choice categorizations are averaged to form a classification image (
              Dotsch &amp; Todorov, 2012; 
              Dotsch et al., 2008, 
              2011; 
              Imhoff et al., 2011). It is notable that if a participant categorized faces randomly, an average of the participant’s responses would produce the base face image, because the counterbalanced visual noise would be canceled out (assuming enough trials are included in the average). If participants respond on some systematic basis (e.g., group membership), then systematic patterns in the pixel intensities will emerge that will reveal the mental representation. It is important to note that the visualization of the mental representation is dependent on the base face and type of noise that is used. Additionally, because there are limitations to the number of trials that can be presented, not all the noise will cancel out. However, averaging across participants provides adequate noise reduction for visualization.
            </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Participant-level classification images</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Custom MATLAB scripts were used to conduct the reverse correlation analysis. First, for each participant, a classification image was created by averaging together all of the parameters of the 450 noise patterns that he or she selected and superimposing the normalized average on the original base image. The images reflected participants’ mental representation of faces belonging to the ingroup or outgroup, as a function of the counterbalanced numerical estimation style factor, the base face, group-specific features, and noise (i.e., error variance).</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Group-level classification images</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Because our theoretical interest concerned the effects of minimal group membership, and not numerical estimation style, ingroup (
                <em>n</em> = 86) and outgroup (
                <em>n</em> = 90) classification images were created by averaging the appropriate participant-level mean parameters and superimposing the normalized average on the original base image (collapsing across the numerical estimation style dimension; see 
                Figure 2).
                <a id="b-fn1"> </a><sup /><br /><br /><a id="fig2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/06324bf3eca670c37475707ba54691ac/5422c463/pdh/psp/psp-106-6-897-fig2a.gif" alt="psp-106-6-897-fig2a.gif" title="Figure 2. Group-level ingroup and outgroup classification images from Study 1." /><em>Figure 2. Group-level ingroup and outgroup classification images from Study 1.</em></span></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Part 2: Assessing impressions of the ingroup and outgroup face representations</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Having implemented the minimal group manipulation in Part 1, which produced visual images representing minimal ingroup and outgroup members, we tested in Part 2 whether ingroup face representations elicited more favorable impressions than outgroup representations, as hypothesized. To this end, an independent sample was recruited to measure trait judgments linked to the ingroup and outgroup face representations estimated during Part 1.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Participants</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">One hundred and nine participants were recruited through the Amazon Mechanical Turk website (
              
<a href="http://www.mturk.com" target="_blank">www.mturk.com</a>) to complete an online survey administered through Qualtrics (
              
<a href="http://www.qualtrics.com" target="_blank">www.qualtrics.com</a>). Participants were given 30 minutes to complete the study and were remunerated with $0.20. On average, participants finished the survey in approximately six minutes. Data from three participants were removed because, based on their Internet Protocol addresses, it appeared they had already completed the survey. Data from five other participants were excluded because their surveys were incomplete. The final data set included 101 participants. Past research suggests that the quality and reliability of data collected on Mechanical Turk are comparable to those of data collected in laboratory settings (
              Buhrmester, Kwang, &amp; Gosling, 2011; 
              Paolacci, Chandler, &amp; Ipeirotis, 2010). All participants were naive to how the faces were generated—no mention was made of numerical estimation or any aspect of group membership.
            </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Procedure</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">After providing informed consent, participants viewed the group-level ingroup and outgroup classification images and rated each on 13 trait dimensions (
              <em>trustworthy, attractive, dominant, caring, sociable, confident, emotionally stable, responsible, intelligent, aggressive, mean, weird,</em> and 
              <em>unhappy</em>; 
              Oosterhof &amp; Todorov, 2008). The two group-level face images were presented adjacent to each other in the upper half of the screen, in counterbalanced positions, in order to draw attention to the comparison of the two images. Ratings were made on a 7-point scale. The order of the trait ratings was randomly determined by the Qualtrics stimulus presentation software.
            </p><a id="psp-106-6-897-EBEDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">For each trait dimension, we conducted paired 
          <em>t</em> tests comparing the ratings of the ingroup and outgroup classification images. The means, 
          <em>t</em> values, and 
          <em>p</em> values for each comparison are presented in 
          Table 1. These tests revealed a striking pattern of results, such that the ingroup face was rated substantially higher than the outgroup face on all the traits considered to be desirable of an ingroup member (
          <em>attractiveness, intelligence, responsibility, confidence, trustworthiness, caring, emotional stability,</em> and 
          <em>sociality</em>). By contrast, the outgroup face representation was judged as significantly more 
          <em>weird</em> than the ingroup face. Ingroup and outgroup face ratings did not differ statistically on 
          <em>dominance, aggressiveness, meanness,</em> and 
          <em>unhappiness</em>.
          
<br /><br /><a id="tbl1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/198818933c63f0fdfdbd8307cee9c79b/5422c463/pdh/psp/psp-106-6-897-tbl1a.gif" alt="psp-106-6-897-tbl1a.gif" title="Means and Paired t-Test Results for the Study 1 Trait Ratings" /><em>Means and Paired t-Test Results for the Study 1 Trait Ratings</em></span></p><a id="psp-106-6-897-EAEDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In Study 1, we investigated whether mere group membership can give rise to contrasting representations of ingroup and outgroup faces, and whether these can then lead to different social impressions. Our results indicate that sharing a minimal group membership with another person can indeed bias facial representations, in a manner that evokes more favorable impressions on a range of traits. Minimally defined ingroup and outgroup faces differed primarily on traits dimensions that signal whether a target should be approached or avoided, but they did not differ on traits found to signify dominance (
          Oosterhof &amp; Todorov, 2008). This pattern comports with the broader literature on minimal group effects, whereby mere categorization evokes an ingroup preference but not necessarily competition or dominance hierarchies (
          Mummendey &amp; Otten, 1998). Although past research has shown that people rely on appearance-related stereotypes when forming impressions (
          Deaux &amp; Lewis, 1984; 
          Kessler &amp; McKenna, 1978; 
          McArthur &amp; Baron, 1983) and that information about a group member’s behavior can trigger assumptions about appearance (
          Dotsch, Wigboldus, &amp; van Knippenberg, 2013), this study is the first to suggest that even seemingly inconsequential group memberships can evoke facial representations that facilitate more favorable impressions of ingroup than outgroup members.
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The use of a reverse correlation task was a particularly notable feature of the study because mental representations of faces are difficult to assess with traditional behavioral techniques. Because the reverse correlation approach, used in Part 1, allows all the features of the representation to vary freely and thus does not make strong assumptions a priori about the informational content of the face, it is able to generate estimates of participants’ facial representations unbiased by the experimenters’ preconceptions. Thus, reverse correlation methodology allowed us to create purely data-driven visualizations of ingroup and outgroup face representations. In Part 2, we then objectively ascertained the impressions elicited by ingroup and outgroup facial representations by asking a naive sample to rate the faces on traits that are highly relevant to face-based evaluations (
          Oosterhof &amp; Todorov, 2008). It was important that we separated the image generation process from the impression formation stage, because without doing so it would have been difficult to mechanistically show that the facial representations can directly contribute to trait inferences.
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Although the reverse correlation approach provided a powerful method for probing the role of facial representations in the minimal group effect, some limitations of this approach should be considered. First, it is unclear whether this representational process occurs spontaneously and in all situations. In all likelihood, there are many situations in which visual representations are not generated or in which group-based preferences are not driven by such representations. Thus, the visual biases identified in Study 1 may represent one of multiple processes that may contribute to minimal group effects. Additionally, it is possible that participants in Part 1 of Study 1 spontaneously generated trait inferences and attitudes that influenced their mental representations of the minimal groups, suggesting a more dynamic interplay of impressions and visual representations. Nevertheless, our experiments showed that once these facial representations are formed, the differences in these representations are themselves sufficient to evoke intergroup bias in trait impressions.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Another potential limitation concerns the effect of the visual clarity of the images produced by the reverse correlation method. For example, it is possible that differences in the clarity of the ingroup and outgroup images contributed to the observed effects, beyond structural differences between these facial images. However, such an effect is difficult to discern from our data because the images used in this study were aggregated across participants. The clarity of the aggregated images could result from different across-group variability for ingroup and outgroup faces. It is additionally possible that image normalization of the noise influenced the pixel intensities across participants in a nonlinear manner complicating the interpretation of clarity differences in the images. Nonetheless, Study 1 served as an initial proof-of-principle that mere group categorization can distort mental representations of faces in a manner that leads to more favorable impressions of ingroup than outgroup members.</p><a id="psp-106-6-897-EDDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_59" title="Study 2">Study 2</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Study 1 provided preliminary evidence that mental representations of faces elicited by minimally defined ingroup and outgroup members can vary and lead to trait impressions that favor the ingroup. The ingroup favoritism evident in these trait ratings suggests that differences in mental representations, based on minimal group membership, could also contribute to prejudiced attitudes (i.e., evaluative positions or associations that are not tied to a particular trait attribute; 
        Amodio &amp; Devine, 2006; 
        Allport, 1954). Therefore, in Study 2, we tested whether the face representations computed in Study 1 on the basis of mere group assignment also elicit ingroup favoritism in attitudes. The strategy of Study 2 was similar to that of Study 1, except that instead of asking an independent sample to explicitly rate the face images on trait dimensions, we recruited an independent sample to complete an implicit measure of attitudes toward the ingroup and outgroup images generated in Part 1 of Study 1.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Additionally, instead of presenting participants with the aggregate ingroup and outgroup classification images, as in Study 1, we used individual participant-level classification images as stimuli in Study 2. Use of the participant-level images allowed for repeated measurements of attitudes associated with the ingroup and outgroup representations, which increased the likelihood that the attitude measure would provide stable effects. The use of participant-level images also precluded potential effects of face variability, which could affect clarity in group-level images, thereby addressing a potential limitation of Study 1.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, given that social attitudes may be expressed without conscious deliberation (
        Devine, 1989; 
        Dovidio, Evans, &amp; Tyler, 1986; 
        Fazio, Jackson, Dunton, &amp; Williams, 1995), use of a measure that implicitly assesses attitudes provided an opportunity to test whether the minimal group effects on face representations can elicit pro-ingroup attitudes at an implicit level of processing.
      </p><a id="psp-106-6-897-ECDDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Participants</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">One hundred and one undergraduates from New York University participated in exchange for course credit and were run simultaneously in individual cubicles in groups of up to four participants.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure and materials</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">After providing consent, participants completed an 
            <em>Affect Misattribution Procedure</em> (AMP; 
            Payne, Cheng, Govorun, &amp; Stewart, 2005), an implicit measure of attitudes. Past work has validated the AMP’s ability to implicitly assess evaluative associations, including those related to racial prejudice, addiction cravings, political ideology, and evocative images (
            Payne et al., 2005; 
            Payne, McClernon, &amp; Dobbins, 2007). The logic of the AMP is that if people are asked to judge an ambiguous target, their evaluation of this target will be biased by the attitudinal information associated with an image that directly preceded that target.
          </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Before they began the AMP, participants were told that the purpose of the study was to measure their responses under distracting conditions and that, on each trial, two images would appear in sequence. The first image would be a grayscale picture and the second image would be a Chinese character. Participants were instructed to do nothing in response to the first image but to respond to the Chinese character by guessing how pleasant or unpleasant the symbol appeared on a scale of 1 to 4 (with 4 being very pleasant). Participants were also told that they should not let the first picture influence their judgment of the symbol. They then completed three practice trials, followed by 176 experimental trials, which used each ingroup and outgroup face image generated by the participants from Part 1 of Study 1 as primes. Trial order was determined randomly according to the DirectRT stimulus presentation software. Additionally, the Chinese characters were randomly chosen on each trial from a set of 200 characters that was downloaded from 
            
<a href="http://www.unc.edu/~bkpayne/materials.html" target="_blank">http://www.unc.edu/~bkpayne/materials.html</a>.
          </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Each trial began with a face (ingroup or outgroup) presented for 75 ms, followed by a blank screen for 125 ms, and then a Chinese character for 100 ms. The character was replaced with a mask consisting of grayscale visual noise, which remained onscreen until a response was registered (following 
            Payne et al., 2005). At the conclusion of the AMP, participants were carefully debriefed. As part of this debriefing, participants indicated whether they could read Chinese characters. Eight participants reported being able to read these characters; their data were excluded from analysis, leaving data from 93 participants.
          </p><a id="psp-106-6-897-EBDDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">We predicted that if faces representing minimal ingroup members trigger more favorable attitudes than those representing outgroup members, Chinese characters should be rated as more pleasant when preceded by an ingroup than an outgroup classification image. To test our prediction, we computed the average pleasantness rating associated with each participant-level classification image. We then conducted a paired 
          <em>t</em> test to examine differences in the pleasantness ratings associated with the ingroup and outgroup images. Consistent with our hypothesis, Chinese characters that followed the ingroup faces (
          <em>M</em> = 2.59, 
          <em>SD</em> = 0.32) were rated more positively than those that followed the outgroup faces (
          <em>M</em> = 2.56, 
          <em>SD</em> = 0.32), 
          <em>t</em>(92) = 2.33, 
          <em>p</em> = .02, 
          <em>d</em> = .09, 95% CI of difference [0.004, 0.056].
          <a id="b-fn2"> </a><sup /></p><a id="psp-106-6-897-EADDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In Study 2, we tested whether differences in mental representations of mere ingroup and outgroup members are sufficient to create pro-ingroup attitudes. The results revealed that the visual renderings of ingroup faces did indeed evoke a more positive attitude than those of outgroup faces, suggesting that subtle differences in the way people visualize ingroup and outgroup members may be sufficient to produce prejudices that favor the ingroup. Although the effect of minimal group assignment on attitudes has been shown in much previous research, our findings show that this effect could, at least in part, be mediated through mental representations of faces.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Our findings also revealed that attitudes elicited by ingroup and outgroup face representations operate implicitly, complementing the observation of explicit impression effects in Study 1. This result is in line with previous research demonstrating that positive ingroup attitudes following mere group categorization can occur implicitly (
          Ashburn-Nardo et al., 2001; 
          Otten &amp; Wentura, 1999; 
          Van Bavel &amp; Cunningham, 2009). Furthermore, this finding is consistent with evidence that brief, even nonconscious, presentations of faces can elicit neural responses indicative of affect (
          de Gelder, Vroomen, Pourtois, &amp; Weiskrantz, 1999; 
          Morris, de Gelder, Weiskrantz, &amp; Dolan, 2001; 
          Whalen et al., 1998; see also 
          Amodio, Harmon-Jones, &amp; Devine, 2003).
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">From a methodological standpoint, it is notable that the results of Study 2 reflected responses to the participant-level images, as opposed to the two group-level images used in Study 1. This finding addresses the concern regarding variability effects on group-level aggregate images and provides converging evidence for the validity of the procedure (as in 
          Dotsch et al., 2008). Furthermore, it is notable that the standard deviations of the AMP responses to ingroup and outgroup participant-level images were close to equivalent. Therefore, it is unlikely that differential variability between the ingroup and outgroup images could explain the observed difference in attitudes.
        </p><a id="psp-106-6-897-ECDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_77" title="Study 3">Study 3</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Having demonstrated that mere group membership alters mental representations of ingroup and outgroup faces in a way that elicits differential trait impressions and attitudes, we aimed in Study 3 to assess the degree to which mental representations of ingroup and outgroup members can influence behavior. Here, we focused on trust behavior, because trust is a central dimension for evaluating faces (
        Oosterhof &amp; Todorov, 2008) and a critical facilitator of harmonious interactions among ingroup members (
        Foddy, Platow, &amp; Yamagishi, 2009; 
        Kramer, 1999; 
        Tanis &amp; Postmes, 2005). To assess the degree to which trust is evoked by ingroup facial representations, in comparison with outgroup representations, we conducted a study in which participants completed an economic trust game with interaction partners represented by the ingroup and outgroup classification images from Study 1 (
        Berg, Dickhaut, &amp; McCabe, 1995). As in Study 2, participants responded to the participant-level classification images.
      </p><a id="psp-106-6-897-ECCDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Participants</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Eighty-one undergraduates from New York University completed the study in exchange for course credit. Up to four participants were run simultaneously in separate cubicles.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Trust game</em></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">After providing informed consent, participants completed a hypothetical trust game with people depicted by the ingroup and outgroup face classification images. Participants were instructed to imagine that they were given $10 on each trial, and that they could choose either to keep this money or to take the chance to increase their share by engaging in a hypothetical economic interaction with individuals who purportedly provided responses in an earlier session. On each interaction trial, participants were given the choice to share a portion of $10 (i.e., $0, $2, $4, $6, $8, or $10). Any money they shared would be quadrupled and given to the interaction partner. The partner would then have the option to return half of the sum to the participant who had shared the money. In this way, it would be possible for the participant to make more money than if he or she had not shared. Hence, the amount of money shared indicates the degree to which the participants trusted the interaction partners.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants played the trust game with 176 different partners, represented only by a photograph. In actuality, these photographs were the participant-level classification images of the ingroup and outgroup faces, presented in a random order across trials. Participants were told that each partner’s face was blurred to protect his or her identity.</p><a id="psp-106-6-897-EBCDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">We predicted that participants would entrust more money to individuals depicted by the ingroup classification images than to those depicted by the outgroup classification images. As a test of this, the average amount entrusted to ingroup faces and outgroup faces was computed for each participant and submitted to a paired 
          <em>t</em> test. This analysis indicated that ingroup face depictions (
          <em>M</em> = $3.01, 
          <em>SD</em> = 1.62) were trusted significantly more than outgroup face depictions (
          <em>M</em> = $2.77, 
          <em>SD</em> = 1.62), 
          <em>t</em>(80) = 6.64, 
          <em>p</em> &lt; .001, 
          <em>d</em> = .15, 95% CI of difference [0.17, 0.30].
          <a id="b-fn3"> </a><sup /></p><a id="psp-106-6-897-EACDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In Study 3, we tested the hypothesis that representations of ingroup and outgroup faces formed on the basis of a minimal group distinction could elicit different patterns of behavior, in the form of trust decisions in an economic game. We found that participants acted in a more trusting manner toward faces reflecting mental representations of ingroup members, as compared with outgroup members, evidencing a pattern of ingroup favoritism in behavior. As in Study 2, the variance in responses to participant-level ingroup and outgroup faces did not differ, further suggesting that the effects were not driven by differential variability in representations of ingroup and outgroup faces. Thus, this study suggests that subtle biases in the way an individual mentally represents the face of an ingroup or outgroup member could result in a difference in trust decisions.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The results of Study 3 add to previous research on trust behavior under minimal group conditions by extracting the effect of minimal group assignment on facial representations and then showing that this information alone can cause differences in trust behavior. In past research (
          Foddy et al., 2009; 
          Platow, Foddy, Yamagishi, Lim, &amp; Chow, 2012), minimal group effects were examined in the absence of group-specific facial information. Given that ingroup favoritism, especially in zero-sum situations, can result in disadvantaging outgroup members, this work suggests a novel pathway through which group-based facial representations can contribute to prejudice, discrimination, and intergroup conflict.
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">It is important to mention that our conclusions are drawn from trust behavior during hypothetical transactions, in the sense that participants could not actually earn money during the game. Differences between hypothetical and real scenarios tend to occur because people risk more money and exhibit self-presentation concerns more strongly during hypothetical situations (
          Ajzen, Brown, &amp; Carvajal, 2004). Although we cannot conclusively rule out the possibility that participants’ behavior would have differed if real money had been exchanged, we believe it is unlikely. First, our procedure should have precluded any self-presentation concerns related to ingroup and outgroup membership, given that participants were unaware of the group-based origins of the face images. Second, although it is possible that participants would have risked less money in their decisions if the money had been real, our interest was in the relative difference in money entrusted to ingroup and outgroup representations rather than the mean amount.
        </p><a id="psp-106-6-897-EBDA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_93" title="Study 4">Study 4</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The aim in Study 4 was to examine, more directly, how facial cues signaling trust contribute to the observed effects of group membership. Because the reverse correlation procedure used to render the images in Study 1 allows for unconstrained representations of facial features, it is sensitive to the many possible ways in which group membership could be instantiated in the physiognomy of a face. For instance, it is possible that trust was conveyed primarily in the eyes or by the shape of the mouth (
        Schul, Mayo, &amp; Burnstein, 2004; 
        Zebrowitz, 1997). Thus, our goal was to objectively test the correspondence between ingroup (vs. outgroup) face representations and an independently produced representation of a trustworthy face. Insight into this physiognomic correspondence would clarify the mechanism underlying the results of Studies 1–3.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">A second, related goal was to examine the extent to which the resemblance between trust and ingroup representations is distributed across the face. This analysis would shed light on whether the ingroup favoritism effects observed thus far are due to specific or gestalt differences between the ingroup and outgroup face representations.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">To address these goals in Study 4, we recruited a new sample of participants to complete a face categorization task. Although similar to that used in Study 1, it pertained to whether a face appears trustworthy. The reverse correlation analysis method was then used to produce visual renderings of participants’ mental images of a trustworthy face. Because people have a relatively strong notion of what a trustworthy face looks like (
        Dotsch &amp; Todorov, 2012), a smaller sample was needed to generate a clear estimate of physiognomic information associated with trust. We then compared the resulting classification image of a 
        <em>trustworthy</em> face (collapsed across subjects) to the group-level classification images from Study 1 that were used to estimate ingroup and outgroup face representations.
      </p><a id="psp-106-6-897-ECBDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Participants</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Fourteen undergraduates from New York University participated in exchange for extra course credit and completed the study in private cubicles, in groups of one to four participants at a time.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The procedure and stimuli were identical to those in Study 1, except that there was no group assignment and, thus, no minimal group paradigm was used. Instead, during the face categorization task, participants were instructed to choose the face from each trial pair 
            <em>that looks the most trustworthy.</em> As was predicted, a visual inspection of the aggregate trust classification image (see 
            Figure 3A) indicated that a sample size of 14 participants was sufficient to generate a clear estimate of participants’ mental image of a trustworthy face. This image closely resembled the trustworthy face classification image generated by 
            Dotsch and Todorov (2012).
            
<br /><br /><a id="fig3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/826c1526fc2d2c7dada6fcb2197e7a3d/5422c463/pdh/psp/psp-106-6-897-fig3a.gif" alt="psp-106-6-897-fig3a.gif" title="Figure 3. (A) Ingroup, outgroup, and trustworthy classification images. (B) Masked classification images with the base face removed used during the Study 4 analyses." /><em>Figure 3. (A) Ingroup, outgroup, and trustworthy classification images. (B) Masked classification images with the base face removed used during the Study 4 analyses.</em></span></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Data reduction</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Participant-level classification images were first created by averaging the 450 faces selected as appearing trustworthy for each participant. A composite mental representation of a trustworthy face was then created by aggregating these participant-level classification images. The classification image depicting the trust representation is presented alongside the Study 1 ingroup and outgroup group-level images in 
            Figure 3A.
          </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">To quantitatively test similarities between the trust- and group-based representations, we correlated the patterns of pixel intensities that composed these classification images. Before calculating the correlations, we removed the base face from all the images, leaving only pixel patterns representing the variation due to participants’ mental images, as inclusion of the base face would have artificially inflated observed correlations between the images. Regions of the image outside the face, including the hair, were masked and excluded from analysis (see 
            Figure 3B to see the masked noise patterns). Finally, for each image, the intensity values of the remaining 89,177 pixels were converted into a single vector and correlated with the other two vectors.
          </p><a id="psp-106-6-897-EBBDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Correlations between the trust and ingroup/outgroup images</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">We hypothesized that the pixel intensities of the trust classification image would be more highly correlated with the ingroup than the outgroup classification image, indicating a greater correspondence between the facial representations of trustworthiness and the ingroup face, as compared with the outgroup face. Supporting our prediction, the vectors representing pixel intensities for the ingroup and trust classification images were significantly correlated (
            <em>r</em> = .46, 
            <em>p</em> &lt; .001). The outgroup and trust classification images were also significantly correlated (
            <em>r</em> = .23, 
            <em>p</em> &lt; .001), but the magnitude of this correlation was significantly lower than that of the trust–ingroup correlation (
            <em>z</em> = 55.56, 
            <em>p</em> &lt; .001). This test confirmed that the resemblance of the ingroup and trust classification images was significantly greater than that of the outgroup and trust classifications.
          </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Given the large sample size (i.e., number of pixels), the significance of the 
            <em>p</em> value is not as notable as the fact that the ingroup classification image accounted for over four times as much variance in the trust classification image as did the outgroup classification image (
            <em>r</em><sup>2</sup> = .21, 95% CI [0.205, 0.215] vs. 
            <em>r</em><sup>2</sup> = .05, 95% CI [0.047, 0.052]). Moreover, if partial correlations are considered to adjust for shared variance between the ingroup and outgroup images, the variance accounted for in the trust image is nine times as large for the ingroup image (
            <em>r</em><sup>2</sup> = .18) as it is for the outgroup image (
            <em>r</em><sup>2</sup> = .02). These results objectively demonstrate that the facial information associated with people’s representation of an ingroup member overlaps more with a mental representation of a trust face than does their representation of an outgroup member.
          </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Region of interest analyses</strong></p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">To assess whether the differences in the correspondences between the ingroup and outgroup faces with the trustworthy face representation were localized or distributed across the face, we conducted the same partial correlation analyses on pixels representing noise patterns within regions of interest (ROIs) covering the eye region, nose, and mouth (see 
            Figure 4). The results from these analyses are summarized in 
            Table 2. Of importance, for each of the regions of interest, the similarity between the trust classification image and the ingroup classification image was significantly larger than the trust classification image and the outgroup classification image. Consistent with research demonstrating that the eyes are especially important for trust judgments (
            Schul et al., 2004; 
            Zebrowitz, 1997), the differences in correlations for the ingroup and outgroup were largest for the eye region, followed by the nose, and then the mouth.
            
<br /><br /><a id="fig4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/ad116066e2f0efe41b46eed4282ed33c/5422c463/pdh/psp/psp-106-6-897-fig4a.gif" alt="psp-106-6-897-fig4a.gif" title="Figure 4. Masks used to conduct the Study 4 region of interest analyses, shown on the trust classification image. The number of pixels in each region of interest were 36,994 (eye region), 8,269 (nose), and 18,251 (mouth)." /><em>Figure 4. Masks used to conduct the Study 4 region of interest analyses, shown on the trust classification image. The number of pixels in each region of interest were 36,994 (eye region), 8,269 (nose), and 18,251 (mouth).</em></span><br /><br /><a id="tbl2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/0b7c121066aa963b4550037113f2cee8/5422c463/pdh/psp/psp-106-6-897-tbl2a.gif" alt="psp-106-6-897-tbl2a.gif" title="Study 4 Partial Correlations Between Trust and Ingroup/Outgroup Classification Images as a Function of Face Region" /><em>Study 4 Partial Correlations Between Trust and Ingroup/Outgroup Classification Images as a Function of Face Region</em></span></p><a id="psp-106-6-897-EABDA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Study 4 examined the hypothesis that differences in facial representations of minimal ingroup and outgroup members could be attributed, at least in part, to the ascription of features associated with trustworthiness to the ingroup face. By comparing patterns in the pixel intensities of the ingroup and outgroup classification images to those of the trust classification image produced by an independent sample, this study provided an especially stringent and objective test of this hypothesis. These results support our theoretical proposal that shared group membership can bias mental representations of faces toward a facial geometry that communicates affiliative signals, which would elicit more favorable impressions, attitudes, and behaviors from perceivers.</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">It is also noteworthy that the ingroup face representation was positively correlated with the trustworthy face presentation, but the outgroup face representation was not negatively correlated with the trustworthy representation. This is consistent with the existing minimal group literature, which has shown that merely separating people into groups increases ingroup favoritism but does not elicit outgroup derogation (
          Mummendey &amp; Otten, 1998).
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, in Study 4, we also examined the ingroup favoritism effect in three regions of the face (the eyes, nose, and mouth). In all three regions, the trust image was correlated more highly with the ingroup image than the outgroup image. This finding suggests that the ingroup bias in mental representation is distributed across multiple regions of the face or, at the very least, is represented in different regions by different participants. Although trust information was communicated in multiple regions of the ingroup face, the effects were strongest for the eyes. This result is consistent with a growing body of research that suggests that the eyes are attended to when making decisions related to trust, possibly because they are assumed to act as a window into the unobservable mind (
          Schul et al., 2004; 
          Zebrowitz, 1997).
        </p><a id="psp-106-6-897-EADA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_116" title="General Discussion">General Discussion</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Over four decades of research has demonstrated that mere group categorization alone is sufficient to cause discriminatory behavior. Simply assigning a person to an arbitrarily defined group produces behavioral and evaluative preferences for the ingroup over the outgroup (e.g., 
        Ashburn-Nardo et al., 2001; 
        Brewer &amp; Silver, 1978; 
        Locksley et al., 1980; 
        Otten &amp; Wentura, 1999; 
        Tajfel et al., 1971; 
        Van Bavel &amp; Cunningham, 2009). In the past several years, numerous studies have investigated how sharing a group identity with others can influence the processing of and memory for their faces. Although this work has revealed that ingroup faces are processed more deeply and accurately than outgroup faces (
        Bernstein et al., 2007; 
        Hugenberg &amp; Corneille, 2009; 
        Ratner &amp; Amodio, 2013; 
        Van Bavel et al., 2008, 
        2011; 
        Young &amp; Hugenberg, 2010), the present research investigated a different mechanism through which mere group categorization can influence face processing. That is, we proposed that mere group assignment can distort mental representations of faces in a way that leads to more favorable responses to ingroup than to outgroup members.
      </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">This hypothesis was examined across four studies. In Study 1, we showed that membership in a minimally defined group was sufficient to produce differences in the facial representations of ingroup and outgroup members and that, when shown to naive participants, a representation of an ingroup face elicited more favorable trait impressions than a representation of an outgroup face. The ingroup facial representation was judged to be more trustworthy, attractive, caring, sociable, emotionally stable, intelligent, responsible, and confident than the outgroup face representation: a pattern of ingroup traits signifying group fitness and affiliative tendencies. Furthermore, we found that ingroup face representations elicited more positive implicitly measured attitudes (Study 2) and trusting behavior (Study 3) than did outgroup face representations. Finally, Study 4 demonstrated that participants’ representation of facial trustworthiness more closely resembled the information contained within an ingroup face representation than an outgroup face representation. Together, these results suggest that people form subtly different mental images of ingroup and outgroup members that can contribute to significant differences in trait impressions, attitudes, and behaviors and that the difference in mental images can be driven, in part, by the ascription of trust-related features to the ingroup face.</p><a id="psp-106-6-897-EDADA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Inferring a Causal Influence of Facial Representations on Intergroup Responses</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">The overarching objective of our work was to study the effects of mere group categorization on facial representations as a means to understand how group memberships can lead to intergroup bias. Although several mechanisms likely contribute to the effect of minimal group assignment on intergroup bias, we examined internal representations of facial information as one causal pathway (cf. 
          Bullock, Green, &amp; Ha, 2010; 
          Green, Ha, &amp; Bullock, 2010). In order to provide inferential traction when multiple mechanisms might exist, methodologists have encouraged experimentally testing each step in a predicted causal path (
          Spencer et al., 2005). In line with this recommendation, we first manipulated the effect of mere group categorization on facial representations (Study 1, Part 1) and then independently assessed the effect of the resultant facial representations on trait impressions (Study 1, Part 2), attitudes (Study 2), and behavior (Study 3). This approach was built upon the already well-established “direct path” between minimal group induction and ingroup favoritism (e.g., 
          Ashburn-Nardo et al., 2001; 
          Brewer &amp; Silver, 1978; 
          Van Bavel &amp; Cunningham, 2009). The use of this design suggests that facial representations can contribute to minimal group effects on attitudes, inferences, and trust decisions.
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">To be clear, our results suggest that biased facial representations can operate as a mechanism leading to intergroup bias, but, like other mediation-based approaches, they do not rule out a role for other mechanisms at any stage of the process. For instance, our work does not preclude the possibility that participants in Part 1 of Study 1 explicitly generated trait inferences as a means to develop mental representations of the minimal groups. It is also possible that clarity differences in the mental images of ingroup and outgroup members contributed to downstream favoritism effects, although the results of Studies 2 and 3 suggest that this is unlikely to be the case. Some people might also use their self-image or prototypical features of other groups they belong to as a basis for discerning their image of a minimally defined ingroup member (see 
          Gramzow, Gaertner, &amp; Sedikides, 2001; 
          Imhoff &amp; Dotsch, 2013, for research consistent with these possibilities). Further research will be required to fully illuminate the range of facial representation mechanisms that contribute to the effects of minimal group distinctions.
        </p><a id="psp-106-6-897-ECADA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Implications for Mental Simulations Involving Ingroup and Outgroup Members</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Our research suggests that facial representations associated with ingroup favoritism may influence social relations. One way this may occur is by guiding mental simulation. Mental simulation refers to people’s ability to transcend the here and now by role-playing past and future experiences (
          Schacter, Addis, &amp; Buckner, 2007; 
          Tulving, 1983). Functionally, mental simulation allows people to plan for and feel emotions associated with scenarios that they are not currently experiencing (
          Marks, 1999; 
          Taylor, Pham, Rivkin, &amp; Armor, 1998).
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">An implication for intergroup interactions is that mental representations of an ingroup face could result in self-fulfilling prophesies (
          Chen &amp; Bargh, 1997; 
          Darley &amp; Fazio, 1980; 
          Merton, 1948). For instance, when anticipating an interaction with an ingroup member, expectations that the target person will look trustworthy, responsible, and sociable could contribute to approach-related attitudes and cooperative behaviors during the interaction, which in turn could increase the likelihood that the ingroup member develops warm attitudes toward the perceiver and reciprocates affiliative behavior. Similar mental simulation effects could also emerge when people are interacting with an ingroup member over e-mail or in other situations that do not provide access to appearance information about the target individual. These self-reinforcing processes could perpetuate good will that promotes positive ingroup relations. Given that outgroup representations elicit less favorable impressions than ingroup representations, outgroup members may be less likely to receive the benefit of the doubt than ingroup members. This would put outgroup members at a further disadvantage.
        </p><a id="psp-106-6-897-EBADA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Implications for Intergroup Face Perception and Self-Regulation</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In most research on minimal group effects on face processing, the guiding assumption is that the ingroup is more motivationally relevant, which leads to deeper and more accurate processing of ingroup faces and relatively superficial processing of outgroup faces (
          Hugenberg &amp; Corneille, 2009; 
          Ratner &amp; Amodio, 2013; 
          Van Bavel et al., 2008, 
          2011; 
          Young &amp; Hugenberg, 2010). This work has been particularly useful for demonstrating the generality of the 
          <em>other race effect</em> (i.e., the finding that same-race faces are remembered better than other-race faces; 
          Malpass &amp; Kravitz, 1969). However, as we have mentioned, in minimal group work outside of the domain of face processing, researchers have suggested that ingroup biases are driven by enhanced positivity toward the ingroup (e.g., 
          Ashburn-Nardo et al., 2001; 
          Brewer &amp; Silver, 1978). Motivated in part by this apparent disconnect in literatures, the current work examined whether people see ingroup members through “rose-colored glasses” and whether these distortions in facial representations contribute to biased intergroup impressions, evaluations, and behaviors. In this way, the perceptual biases promoted by these mental representations provide a mechanism for facilitating one’s goal to favor ingroup members (i.e., an example of how motivated perception serves self-regulation; 
          Amodio, 2010).
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Our findings also raise the possibility that internally generated representations of ingroup and outgroup faces influence bottom-up processes through which faces are attended to and perceived. Predictive coding theories state that expectations can guide the acquisition of visual information (
          Friston, 2005; 
          Mumford, 1992; 
          Summerfield et al., 2006). From this perspective, it is possible that if people expect an ingroup member to have certain facial characteristics but the person’s appearance diverges from this prediction, this discordant information would receive more processing. As such, formal models could be developed that use visual renderings of ingroup and outgroup face representations as Bayesian priors for predicting the duration and location of endogenous attention and eye saccades allocated to processing attributes of a particular face. This type of an analysis could advance theories of how mere group knowledge biases the perceptual information that reaches the retina and is subsequently encoded in the visual cortex. Moreover, such work may lead to the development of strategies that protect against this type of bias by changing the predictive visual codes that people use to disambiguate facial information of group members.
        </p><a id="psp-106-6-897-EAADA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Conclusion</h4><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Although the role of appearance cues in group-based processes had been emphasized in seminal theoretical articles (e.g., 
          Brewer, 1988; 
          Carlston, 1992; 
          McArthur &amp; Baron, 1983), until recently, the bulk of social cognition research overlooked the contributions of facial information (Macrae &amp; Bodenhausen, 2006; 
          Zebrowitz, 2006). Our studies contribute to the recent movement to reintegrate research on visual face processing into the social psychological understanding of intergroup responses (e.g., 
          Dotsch et al., 2008; 
          Hugenberg &amp; Corneille, 2009; 
          Ito &amp; Urland, 2005; 
          Kaul, Ratner, &amp; Van Bavel, 2014; 
          Ofan, Rubin, &amp; Amodio, 2011, 
          in press; 
          Ratner &amp; Amodio, 2013; 
          Ratner, Kaul, &amp; Van Bavel, 2013; 
          Van Bavel et al., 2011).
        </p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">In this spirit, our work provides evidence that mere group categorization can lead to ingroup favoritism through effects on mental representations of faces. Whereas past research had shown that simply separating people into novel groups triggers biases that favor ingroup members, it had not been previously established that representations of faces could contribute to this phenomenon. Given that facial representations are utilized to make sense of other people, this research provides insight into a previously underexplored route through which group-based biases can influence society.</p><a id="psp-106-6-897-R" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_132" title="Footnotes">Footnotes</a></span><a id="fn1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup>1</sup> The ingroup image was averaged across 44 overestimator and 42 underestimator participant-level classification images. The outgroup image was averaged across 42 overestimator and 48 underestimator images.</p><a id="fn2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup>2</sup> We reported the results for a paired 
          <em>t</em> test collapsing across the numerical estimation style dimension because we were interested in group differences. However, because our analyses for this study used the participant-level images, and we thus had information about the group and numerical estimation style for each image, we were also able to conduct a 2 (group: ingroup vs. outgroup) × 2 (numerical estimation style: overestimator vs. underestimator) repeated measures analysis of variance (ANOVA). Consistent with the results of our paired 
          <em>t</em> test, this analysis produced a main effect of group, such that, irrespective of numerical estimation style, Chinese characters that followed the ingroup faces (
          <em>M</em> = 2.59, 
          <em>SD</em> = 0.32) were rated more positively than those that followed the outgroup faces (
          <em>M</em> = 2.56, 
          <em>SD</em> = 0.32), 
          <em>F</em>(1, 92) = 5.33, 
          <em>p</em> = .02, η
          p<sup>2</sup> = .06. There were no numerical estimation style or interaction effects (
          <em>p</em>s &lt; .31).
        </p><a id="fn3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup>3</sup> As with Study 2, the analyses for Study 3 used the participant-level images. Therefore, we were also able to conduct a 2 (group: ingroup vs. outgroup) × 2 (numerical estimation style: overestimator vs. underestimator) repeated measures ANOVA. In line with the results from the paired 
          <em>t</em> test, this analysis produced a significant main effect for group, such that ingroup face depictions (
          <em>M</em> = 3.01, 
          <em>SD</em> = 1.62) were trusted significantly more than outgroup face depictions (
          <em>M</em> = 2.77, 
          <em>SD</em> = 1.62), 
          <em>F</em>(1, 80) = 44.02, 
          <em>p</em> &lt; .001, η
          p<sup>2</sup> = .36. Of interest, and unexpectedly, a significant main effect also emerged for numerical estimation style, indicating that overestimators (
          <em>M</em> = 3.01, 
          <em>SD</em> = 1.61) were generally trusted significantly more than underestimators (
          <em>M</em> = 2.76, 
          <em>SD</em> = 1.63), 
          <em>F</em>(1, 80) = 37.28, 
          <em>p</em> &lt; .001, η
          p<sup>2</sup> = .32. These main effects were qualified by a significant interaction, 
          <em>F</em>(1, 80) = 10.46, 
          <em>p</em> &lt; .01, η
          p<sup>2</sup> = .12, which revealed that the group effect was larger in response to faces representing underestimators (
          <em>M</em><em>(ingroup – outgroup)</em> = .33, 
          <em>SD</em><em>(ingroup – outgroup)</em> = .40) than to faces representing overestimators (
          <em>M</em><em>(ingroup-outgroup)</em> = .11, 
          <em>SD</em><em>(ingroup-outgroup)</em> = .46), 
          <em>t</em>(80) = 3.24, 
          <em>p</em> = .002, 
          <em>d</em> = .51. It is notable, however, that the ingroup was trusted significantly more than the outgroup for both the overestimators, 
          <em>t</em>(80) = 2.14, 
          <em>p</em> = .04, 
          <em>d</em> = .07, and the underestimators, 
          <em>t</em>(80) = 7.56, 
          <em>p</em> &lt; .001, 
          <em>d</em> = .20. Thus, our hypothesis about the effect of group membership was supported independently of the effect observed for the numerical estimation style factor.
        </p><a id="psp-106-6-897-E0YD0ACA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link_" href="#toc" id="hd_toc_139" title="References">References</a></span><a id="c1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ahumada, 
              A. J. (
          2002). 
          Classification image weights and internal noise level estimation. 
          <em>Journal of Vision</em>, 
          <em>2</em>, 
          121–
          131. doi:
          10.1167/2.1.8</p><a id="c2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ahumada, 
              A., &amp; 
            Lovell, 
              J. (
          1971). 
          Stimulus features in signal detection. 
          <em>Journal of the Acoustical Society of America</em>, 
          <em>49</em>, 
          1751–
          1756. doi:
          10.1121/1.1912577</p><a id="c3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ajzen, 
              I., 
            Brown, 
              T. C., &amp; 
            Carvajal, 
              F. (
          2004). 
          Explaining the discrepancy between intentions and actions: The case of hypothetical bias in contingent valuation. 
          <em>Personality and Social Psychology Bulletin</em>, 
          <em>30</em>, 
          1108–
          1121. doi:
          10.1177/0146167204264079</p><a id="c125" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Allport, 
              G. W. (
          1954). 
          <em>The nature of prejudice</em>. 
          Reading, MA: 
          Addison-Wesley.
        </p><a id="c4" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Amodio, 
              D. M. (
          2010). 
          Coordinated roles of motivation and perception in the regulation of intergroup responses: Frontal cortical asymmetry effects on the P2 event-related potential and behavior. 
          <em>Journal of Cognitive Neuroscience</em>, 
          <em>22</em>, 
          2609–
          2617. doi:
          10.1162/jocn.2009.21395</p><a id="c5" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Amodio, 
              D. M., &amp; 
            Devine, 
              P. G. (
          2006). 
          Stereotyping and evaluation in implicit race bias: Evidence for independent constructs and unique effects on behavior. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>91</em>, 
          652–
          661. doi:
          10.1037/0022-3514.91.4.652</p><a id="c6" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Amodio, 
              D. M., 
            Harmon-Jones, 
              E., &amp; 
            Devine, 
              P. G. (
          2003). 
          Individual differences in the activation and control of affective race bias as assessed by startle eyeblink responses and self-report. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>84</em>, 
          738–
          753. doi:
          10.1037/0022-3514.84.4.738</p><a id="c7" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Andersen, 
              S. M., &amp; 
            Klatzky, 
              R. L. (
          1987). 
          Traits and social stereotypes: Levels of categorization in person perception. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>53</em>, 
          235–
          246. doi:
          10.1037/0022-3514.53.2.235</p><a id="c8" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ashburn-Nardo, 
              L., 
            Voils, 
              C. I., &amp; 
            Monteith, 
              M. J. (
          2001). 
          Implicit associations as the seeds of intergroup bias: How easily do they take root?<em>Journal of Personality and Social Psychology</em>, 
          <em>81</em>, 
          789–
          799. doi:
          10.1037/0022-3514.81.5.789</p><a id="c9" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Beaupré, 
              M. G., &amp; 
            Hess, 
              U. (
          2003). 
          In my mind, my friend smiles: A case of in-group favoritism. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>39</em>, 
          371–
          377. doi:
          10.1016/S0022-1031(03)00012-X</p><a id="c10" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Bentin, 
              S., 
            Allison, 
              T., 
            Puce, 
              A., 
            Perez, 
              E., &amp; 
            McCarthy, 
              G. (
          1996). 
          Electrophysiological studies of face perception in humans. 
          <em>Journal of Cognitive Neuroscience</em>, 
          <em>8</em>, 
          551–
          565. doi:
          10.1162/jocn.1996.8.6.551</p><a id="c11" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Berg, 
              J., 
            Dickhaut, 
              J., &amp; 
            McCabe, 
              K. (
          1995). 
          Trust, reciprocity, and social history. 
          <em>Games and Economic Behavior</em>, 
          <em>10</em>, 
          122–
          142. doi:
          10.1006/game.1995.1027</p><a id="c12" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Bernstein, 
              M. J., 
            Young, 
              S. G., &amp; 
            Hugenberg, 
              K. (
          2007). 
          The cross-category effect: Mere social categorization is sufficient to elicit an own-group bias in face recognition. 
          <em>Psychological Science</em>, 
          <em>18</em>, 
          706–
          712. doi:
          10.1111/j.1467-9280.2007.01964.x</p><a id="c13" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Birmingham, 
              E., 
            Bischof, 
              W. F., &amp; 
            Kingstone, 
              A. (
          2009a). 
          Get real! Resolving the debate about equivalent social stimuli. 
          <em>Visual Cognition</em>, 
          <em>17</em>, 
          904–
          924. doi:
          10.1080/13506280902758044</p><a id="c14" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Birmingham, 
              E., 
            Bischof, 
              W. F., &amp; 
            Kingstone, 
              A. (
          2009b). 
          Saliency does not account for fixations to eyes within social scenes. 
          <em>Vision Research</em>, 
          <em>49</em>, 
          2992–
          3000. doi:
          10.1016/j.visres.2009.09.014</p><a id="c15" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Blair, 
              I. V., 
            Judd, 
              C. M., &amp; 
            Chapleau, 
              K. M. (
          2004). 
          The influence of Afrocentric facial features in criminal sentencing. 
          <em>Psychological Science</em>, 
          <em>15</em>, 
          674–
          679. doi:
          10.1111/j.0956-7976.2004.00739.x</p><a id="c16" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Blair, 
              I. V., 
            Ma, 
              J. E., &amp; 
            Lenton, 
              A. P. (
          2001). 
          Imagining stereotypes away: The moderation of implicit stereotypes through mental imagery. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>81</em>, 
          828–
          841. doi:
          10.1037/0022-3514.81.5.828</p><a id="c135" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Bodenhausen, 
              G. V., &amp; 
            Macrae, 
              C. N. (
          2006). 
          Putting a face on person perception. 
          <em>Social Cognition</em>, 
          <em>24</em>, 
          511–
          515.
        </p><a id="c17" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Brewer, 
              M. B. (
          1988). 
          A dual process model of impression formation. In 
          T.Srull &amp; 
            R.Wyer (
          Eds.), 
          <em>Advances in social cognition</em> (Vol. 
          <em>1</em>, pp. 
          1–
          36). 
          Hillsdale, NJ: 
          Erlbaum.
        </p><a id="c18" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Brewer, 
              M. B. (
          1999). 
          The psychology of prejudice: Ingroup love or outgroup hate?<em>Journal of Social Issues</em>, 
          <em>55</em>, 
          429–
          444. doi:
          10.1111/0022-4537.00126</p><a id="c19" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Brewer, 
              M. B., &amp; 
            Silver, 
              M. (
          1978). 
          Ingroup bias as a function of task characteristics. 
          <em>European Journal of Social Psychology</em>, 
          <em>8</em>, 
          393–
          400. doi:
          10.1002/ejsp.2420080312</p><a id="c20" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Brown, 
              J. D., 
            Collins, 
              R. L., &amp; 
            Schmidt, 
              G. W. (
          1988). 
          Self-esteem and direct versus indirect forms of self-enhancement. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>55</em>, 
          445–
          453. doi:
          10.1037/0022-3514.55.3.445</p><a id="c21" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Buhrmester, 
              M., 
            Kwang, 
              T., &amp; 
            Gosling, 
              S. D. (
          2011). 
          Amazon’s Mechanical Turk: A new source of inexpensive, yet high-quality, data?<em>Perspectives on Psychological Science</em>, 
          <em>6</em>, 
          3–
          5. doi:
          10.1177/1745691610393980</p><a id="c22" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Bullock, 
              J. G., 
            Green, 
              D. P., &amp; 
            Ha, 
              S. E. (
          2010). 
          Yes, but what’s the mechanism? (don’t expect an easy answer). 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>98</em>, 
          550–
          558. doi:
          10.1037/a0018933</p><a id="c23" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Carlston, 
              D. E. (
          1992). 
          Impression formation and the modular mind: The associated systems theory. In 
          L. L.Martin &amp; 
            A.Tesser (
          Eds.), 
          <em>The construction of social judgments</em> (pp. 
          301–
          341). 
          Hillsdale, NJ: 
          Erlbaum.
        </p><a id="c24" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Castano, 
              E., 
            Yzerbyt, 
              V., 
            Bourguignon, 
              D., &amp; 
            Seron, 
              E. (
          2002). 
          Who may enter? The impact of in-group identification on in-group/out-group categorization. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>38</em>, 
          315–
          322. doi:
          10.1006/jesp.2001.1512</p><a id="c25" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Chen, 
              M., &amp; 
            Bargh, 
              J. A. (
          1997). 
          Nonconscious behavioral confirmation processes: The self-fulfilling consequences of automatic stereotype activation. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>33</em>, 
          541–
          560. doi:
          10.1006/jesp.1997.1329</p><a id="c26" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Crisp, 
              R. J., &amp; 
            Turner, 
              R. N. (
          2009). 
          Can imagined interactions produce positive perceptions? Reducing prejudice through simulated social contact. 
          <em>American Psychologist</em>, 
          <em>64</em>, 
          231–
          240. doi:
          10.1037/a0014718</p><a id="c27" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Darley, 
              J. M., &amp; 
            Fazio, 
              R. H. (
          1980). 
          Expectancy confirmation processes arising in the social interaction sequence. 
          <em>American Psychologist</em>, 
          <em>35</em>, 
          867–
          881. doi:
          10.1037/0003-066X.35.10.867</p><a id="c28" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Deaux, 
              K., &amp; 
            Lewis, 
              L. L. (
          1984). 
          Structure of gender stereotypes: Interrelationships among components and gender label. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>46</em>, 
          991–
          1004. doi:
          10.1037/0022-3514.46.5.991</p><a id="c29" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">de Gelder, 
              B., 
            Vroomen, 
              J., 
            Pourtois, 
              G., &amp; 
            Weiskrantz, 
              L. (
          1999). 
          Non-conscious recognition of affect in the absence of striate cortex. 
          <em>NeuroReport</em>, 
          <em>10</em>, 
          3759–
          3763. doi:
          10.1097/00001756-199912160-00007</p><a id="c30" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Devine, 
              P. G. (
          1989). 
          Stereotypes and prejudice: Their automatic and controlled components. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>56</em>, 
          5–
          18. doi:
          10.1037/0022-3514.56.1.5</p><a id="c136" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Doise, 
              W., 
            Csepeli, 
              G., 
            Dann, 
              H. D., 
            Gouge, 
              C., 
            Larsen, 
              K., &amp; 
            Ostell, 
              A. (
          1972). 
          An experimental investigation into the formation of intergroup representations. 
          <em>European Journal of Social Psychology</em>, 
          <em>2</em>, 
          202–
          204. doi:
          10.1002/ejsp.2420020208</p><a id="c32" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dotsch, 
              R., &amp; 
            Todorov, 
              A. (
          2012). 
          Reverse correlating social face perception. 
          <em>Social Psychological &amp; Personality Science</em>, 
          <em>3</em>, 
          562–
          571. doi:
          10.1177/1948550611430272</p><a id="c33" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dotsch, 
              R., 
            Wigboldus, 
              D. H. J., 
            Langner, 
              O., &amp; 
            van Knippenberg, 
              A. (
          2008). 
          Ethnic out-group faces are biased in the prejudiced mind. 
          <em>Psychological Science</em>, 
          <em>19</em>, 
          978–
          980. doi:
          10.1111/j.1467-9280.2008.02186.x</p><a id="c34" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dotsch, 
              R., 
            Wigboldus, 
              D. H. J., &amp; 
            van Knippenberg, 
              A. (
          2011). 
          Biased allocation of faces to social categories. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>100</em>, 
          999–
          1014. doi:
          10.1037/a0023026</p><a id="c35" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dotsch, 
              R., 
            Wigboldus, 
              D. H. J., &amp; 
            van Knippenberg, 
              A. (
          2013). 
          Behavioral information biases the expected facial appearance of members of novel groups. 
          <em>European Journal of Social Psychology</em>, 
          <em>43</em>, 
          116–
          125. doi:
          10.1002/ejsp.1928</p><a id="c36" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dovidio, 
              J. F., 
            Evans, 
              N., &amp; 
            Tyler, 
              R. B. (
          1986). 
          Racial stereotypes: The contents of their cognitive representations. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>22</em>, 
          22–
          37. doi:
          10.1016/0022-1031(86)90039-9</p><a id="c37" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Dunham, 
              Y. (
          2011). 
          An angry = outgroup effect. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>47</em>, 
          668–
          671. doi:
          10.1016/j.jesp.2011.01.003</p><a id="c38" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Eberhardt, 
              J. L., 
            Davies, 
              P. G., 
            Purdie-Vaughns, 
              V. J., &amp; 
            Johnson, 
              S. L. (
          2006). 
          Looking deathworthy: Perceived stereotypicality of Black defendants predicts capital-sentencing outcomes. 
          <em>Psychological Science</em>, 
          <em>17</em>, 
          383–
          386. doi:
          10.1111/j.1467-9280.2006.01716.x</p><a id="c39" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Fazio, 
              R. H., 
            Jackson, 
              J. R., 
            Dunton, 
              B. C., &amp; 
            Williams, 
              C. J. (
          1995). 
          Variability in automatic activation as an unobstrusive measure of racial attitudes: A bona fide pipeline?<em>Journal of Personality and Social Psychology</em>, 
          <em>69</em>, 
          1013–
          1027. doi:
          10.1037/0022-3514.69.6.1013</p><a id="c40" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Foddy, 
              M., 
            Platow, 
              M. J., &amp; 
            Yamagishi, 
              T. (
          2009). 
          Group-based trust in strangers. 
          <em>Psychological Science</em>, 
          <em>20</em>, 
          419–
          422. doi:
          10.1111/j.1467-9280.2009.02312.x</p><a id="c41" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Friston, 
              K. (
          2005). 
          A theory of cortical responses. 
          <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em>, 
          <em>360</em>, 
          815–
          836. doi:
          10.1098/rstb.2005.1622</p><a id="c42" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Gerard, 
              H. B., &amp; 
            Hoyt, 
              M. F. (
          1974). 
          Distinctiveness of social categorization and attitude toward ingroup members. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>29</em>, 
          836–
          842. doi:
          10.1037/h0036204</p><a id="c43" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Goren, 
              C. C., 
            Sarty, 
              M., &amp; 
            Wu, 
              P. Y. K. (
          1975). 
          Visual following and pattern discrimination of face-like stimuli by newborn infants. 
          <em>Pediatrics</em>, 
          <em>56</em>, 
          544–
          549.
        </p><a id="c44" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Gosselin, 
              F., &amp; 
            Schyns, 
              P. G. (
          2003). 
          Superstitious perceptions reveal properties of internal representations. 
          <em>Psychological Science</em>, 
          <em>14</em>, 
          505–
          509. doi:
          10.1111/1467-9280.03452</p><a id="c45" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Gramzow, 
              R. H., 
            Gaertner, 
              L., &amp; 
            Sedikides, 
              C. (
          2001). 
          Memory for ingroup and outgroup information in a minimal group context: The self as an informational base. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>80</em>, 
          188–
          205. doi:
          10.1037/0022-3514.80.2.188</p><a id="c46" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Green, 
              D. P., 
            Ha, 
              S. E., &amp; 
            Bullock, 
              J. G. (
          2010). 
          Enough already about “black box” experiments: Studying mediation is more difficult than most scholars suppose. 
          <em>Annals of the American Academy of Political and Social Science</em>, 
          <em>628</em>, 
          200–
          208. doi:
          10.1177/0002716209351526</p><a id="c47" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Hassin, 
              R., &amp; 
            Trope, 
              Y. (
          2000). 
          Facing faces: Studies on the cognitive aspects of physiognomy. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>78</em>, 
          837–
          852. doi:
          10.1037/0022-3514.78.5.837</p><a id="c48" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Haxby, 
              J. V., 
            Hoffman, 
              E. A., &amp; 
            Gobbini, 
              M. I. (
          2000). 
          The distributed human neural system for face perception. 
          <em>Trends in Cognitive Sciences</em>, 
          <em>4</em>, 
          223–
          233. doi:
          10.1016/S1364-6613(00)01482-0</p><a id="c49" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Hugenberg, 
              K., &amp; 
            Corneille, 
              O. (
          2009). 
          Holistic processing is tuned for in-group faces. 
          <em>Cognitive Science</em>, 
          <em>33</em>, 
          1173–
          1181. doi:
          10.1111/j.15551-6709.2009.01048.x</p><a id="c50" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Imhoff, 
              R., &amp; 
            Dotsch, 
              R. (
          2013). 
          Do we look like me or like us? Visual projection as self- or ingroup-projection. 
          <em>Social Cognition</em>, 
          <em>31</em>, 
          806–
          816. doi:
          10.1521/soco.2013.31.6.806</p><a id="c51" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Imhoff, 
              R., 
            Dotsch, 
              R., 
            Bianchi, 
              M., 
            Banse, 
              R., &amp; 
            Wigboldus, 
              D. H. (
          2011). 
          Facing Europe: Visualizing spontaneous in-group projection. 
          <em>Psychological Science</em>, 
          <em>22</em>, 
          1583–
          1590. doi:
          10.1177/0956797611419675</p><a id="c52" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ishai, 
              A., 
            Haxby, 
              J. V., &amp; 
            Ungerleider, 
              L. G. (
          2002). 
          Visual imagery of famous faces: Effects of memory and attention revealed by fMRI. 
          <em>NeuroImage</em>, 
          <em>17</em>, 
          1729–
          1741. doi:
          10.1006/nimg.2002.1330</p><a id="c137" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ito, 
              T. A., &amp; 
            Urland, 
              G. R. (
          2005). 
          The influence of processing objectives on the perception of faces: An ERP study of race and gender perception. 
          <em>Cognitive, Affective, and Behavioral Neuroscience</em>, 
          <em>5</em>, 
          21–
          36.
        </p><a id="c53" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Jack, 
              R. E., 
            Caldara, 
              R., &amp; 
            Schyns, 
              P. G. (
          2012). 
          Internal representations reveal cultural diversity in expectations of facial expressions of emotion. 
          <em>Journal of Experimental Psychology: General</em>, 
          <em>141</em>, 
          19–
          25. doi:
          10.1037/a0023463</p><a id="c54" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Johnson, 
              M. H., &amp; 
            Morton, 
              J. (
          1991). 
          <em>Biology and cognitive development: The case of face recognition</em>. 
          Oxford, England: 
          Blackwell.
        </p><a id="c55" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Johnson, 
              R. W. (
          1981). 
          Perceived physical attractiveness of supporters of Canada’s political parties: Stereotype or in-group bias?<em>Canadian Journal of Behavioural Science/Revue canadienne des sciences du comportement</em>, 
          <em>13</em>, 
          320–
          325. doi:
          10.1037/h0081203</p><a id="c56" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Kanwisher, 
              N., 
            McDermott, 
              J., &amp; 
            Chun, 
              M. M. (
          1997). 
          The fusiform face area: A module in human extrastriate cortex specialized for face perception. 
          <em>Journal of Neuroscience</em>, 
          <em>17</em>, 
          4302–
          4311.
        </p><a id="c57" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Karremans, 
              J. C., 
            Dotsch, 
              R., &amp; 
            Corneille, 
              O. (
          2011). 
          Romantic relationship status biases memory of faces of attractive opposite-sex others: Evidence from a reverse-correlation paradigm. 
          <em>Cognition</em>, 
          <em>121</em>, 
          422–
          426. doi:
          10.1016/j.cognition.2011.07.008</p><a id="c59" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Kaul, 
              C., 
            Ratner, 
              K. G., &amp; 
            Van Bavel, 
              J. J. (
          2014). 
          Dynamic representations of race: Processing goals shape race encoding in the fusiform gyri. 
          <em>Social Cognitive and Affective Neuroscience</em>, 
          <em>9</em>, 
          326–
          332. doi:
          10.1093/scan/nss138</p><a id="c60" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Kessler, 
              S. J., &amp; 
            McKenna, 
              W. (
          1978). 
          <em>Gender: An ethnomethodological approach</em>. 
          New York, NY: 
          Wiley.
        </p><a id="c61" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Kramer, 
              R. M. (
          1999). 
          Trust and distrust in organizations: Emerging perspectives, enduring questions. 
          <em>Annual Review of Psychology</em>, 
          <em>50</em>, 
          569–
          598. doi:
          10.1146/annurev.psych.50.1.569</p><a id="c62" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Krosch, 
              A. R., 
            Berntsen, 
              L., 
            Amodio, 
              D. M., 
            Jost, 
              J. T., &amp; 
            Van Bavel, 
              J. J. (
          2013). 
          On the ideology of hypodescent: Political conservatism predicts categorization of racially ambiguous faces as Black. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>49</em>, 
          1196–
          1203. doi:
          10.1016/j.jesp.2013.05.009</p><a id="c63" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Leyens, 
              J., &amp; 
            Yzerbyt, 
              V. Y. (
          1992). 
          The ingroup overexclusion effect: Impact of valence and confirmation on stereotypical information search. 
          <em>European Journal of Social Psychology</em>, 
          <em>22</em>, 
          549–
          569. doi:
          10.1002/ejsp.2420220604</p><a id="c65" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Livingston, 
              R. W., &amp; 
            Brewer, 
              M. B. (
          2002). 
          What are we really priming? Cue-based versus category-based processing of facial stimuli. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>82</em>, 
          5–
          18. doi:
          10.1037/0022-3514.82.1.5</p><a id="c66" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Locksley, 
              A., 
            Ortiz, 
              V., &amp; 
            Hepburn, 
              C. (
          1980). 
          Social categorization and discriminatory behavior: Extinguishing the minimal intergroup discrimination effect. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>39</em>, 
          773–
          783. doi:
          10.1037/0022-3514.39.5.773</p><a id="c67" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Lundqvist, 
              D., &amp; 
            Litton, 
              J. E. (
          1998). 
          <em>The averaged Karolinska directed emotional faces—AKDEF</em> [
          CD-ROM]. 
          Stockholm, Sweden: 
          Psychology Section, Karolinska Institutet.
        </p><a id="c68" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ma, 
              D. S., &amp; 
            Correll, 
              J. (
          2011). 
          Target prototypicality moderates racial bias in the decision to shoot. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>47</em>, 
          391–
          396. doi:
          10.1016/j.jesp.2010.11.002</p><a id="c69" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Macrae, 
              C. N., &amp; 
            Bodenhausen, 
              G. V. (
          2000). 
          Social cognition: Thinking categorically about others. 
          <em>Annual Review of Psychology</em>, 
          <em>51</em>, 
          93–
          120. doi:
          10.1146/annurev.psych.51.1.93</p><a id="c70" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Macrae, 
              C. N., 
            Quinn, 
              K. A., 
            Mason, 
              M. F., &amp; 
            Quadflieg, 
              S. (
          2005). 
          Understanding others: The face and person construal. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>89</em>, 
          686–
          695. doi:
          10.1037/0022-3514.89.5.686</p><a id="c71" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Malpass, 
              R. S., &amp; 
            Kravitz, 
              J. (
          1969). 
          Recognition for faces of own and other race. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>13</em>, 
          330–
          334. doi:
          10.1037/h0028434</p><a id="c72" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Mangini, 
              M. C., &amp; 
            Biederman, 
              I. (
          2004). 
          Making the ineffable explicit: Estimating the information employed for face classifications. 
          <em>Cognitive Science</em>, 
          <em>28</em>, 
          209–
          226. doi:
          10.1207/s15516709cog2802_4</p><a id="c73" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Marks, 
              D. F. (
          1999). 
          Consciousness, mental imagery and action. 
          <em>British Journal of Psychology</em>, 
          <em>90</em>, 
          567–
          585. doi:
          10.1348/000712699161639</p><a id="c74" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Martin-Malivel, 
              J., 
            Mangini, 
              M. C., 
            Fagot, 
              J., &amp; 
            Biederman, 
              I. (
          2006). 
          Do humans and baboons use the same information when categorizing human and baboon faces?<em>Psychological Science</em>, 
          <em>17</em>, 
          599–
          607. doi:
          10.1111/j.1467-9280.2006.01751.x</p><a id="c75" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">McArthur, 
              L. Z., &amp; 
            Baron, 
              R. M. (
          1983). 
          Toward an ecological theory of social perception. 
          <em>Psychological Review</em>, 
          <em>90</em>, 
          215–
          238. doi:
          10.1037/0033-295X.90.3.215</p><a id="c76" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Mechelli, 
              A., 
            Price, 
              C. J., 
            Friston, 
              K. J., &amp; 
            Ishai, 
              A. (
          2004). 
          Where bottom-up meets top-down: Neuronal interactions during perception and imagery. 
          <em>Cerebral Cortex</em>, 
          <em>14</em>, 
          1256–
          1265. doi:
          10.1093/cercor/bhh087</p><a id="c77" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Merton, 
              R. K. (
          1948). 
          The self-fulfilling prophecy. 
          <em>Antioch Review</em>, 
          <em>8</em>, 
          193–
          210. doi:
          10.2307/4609267</p><a id="c78" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Morris, 
              J. S., 
            de Gelder, 
              B., 
            Weiskrantz, 
              L., &amp; 
            Dolan, 
              R. J. (
          2001). 
          Differential extrageniculostriate and amygdala responses to presentation of emotional faces in a cortically blind field. 
          <em>Brain</em>, 
          <em>124</em>, 
          1241–
          1252. doi:
          10.1093/brain/124.6.1241</p><a id="c79" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Mumford, 
              D. (
          1992). 
          On the computational architecture of the neocortex. 
          <em>Biological Cybernetics</em>, 
          <em>66</em>, 
          241–
          251. doi:
          10.1007/BF00198477</p><a id="c80" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Mummendey, 
              A., &amp; 
            Otten, 
              S. (
          1998). 
          Positive–negative asymmetry in social discrimination. 
          <em>European Review of Social Psychology</em>, 
          <em>9</em>, 
          107–
          143. doi:
          10.1080/14792779843000063</p><a id="c81" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Mussweiler, 
              T., 
            Gabriel, 
              S., &amp; 
            Bodenhausen, 
              G. V. (
          2000). 
          Shifting social identities as a strategy for deflecting threatening social comparisons. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>79</em>, 
          398–
          409. doi:
          10.1037/0022-3514.79.3.398</p><a id="c82" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">O’Craven, 
              K. M., &amp; 
            Kanwisher, 
              N. (
          2000). 
          Mental imagery of faces and places activates corresponding stimulus-specific brain regions. 
          <em>Journal of Cognitive Neuroscience</em>, 
          <em>12</em>, 
          1013–
          1023. doi:
          10.1162/08989290051137549</p><a id="c83" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ofan, 
              R. H., 
            Rubin, 
              N., &amp; 
            Amodio, 
              D. M. (
          2011). 
          Seeing race: N170 responses to race and their relation to automatic racial attitudes and controlled processing. 
          <em>Journal of Cognitive Neuroscience</em>, 
          <em>23</em>, 
          3153–
          3161. doi:
          10.1162/jocn_a_00014</p><a id="c84" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ofan, 
              R. H., 
            Rubin, 
              N., &amp; 
            Amodio, 
              D. M. (
          in press). 
          Situation-based social anxiety enhances the neural processing of faces: Evidence from an intergroup context. 
          <em>Social Cognitive and Affective Neuroscience</em>. doi:
          10.1093/scan/nst087</p><a id="c85" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Oosterhof, 
              N. N., &amp; 
            Todorov, 
              A. (
          2008). 
          The functional basis of face evaluation. 
          <em>Proceedings of the National Academy of Sciences, USA</em>, 
          <em>105</em>, 
          11087–
          11092. doi:
          10.1073/pnas.0805664105</p><a id="c86" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Otten, 
              S., &amp; 
            Wentura, 
              D. (
          1999). 
          About the impact of automaticity in the minimal group paradigm: Evidence from affective priming tasks. 
          <em>European Journal of Social Psychology</em>, 
          <em>29</em>, 
          1049–
          1071. doi:
          10.1002/(SICI)1099-0992(199912)29:8&lt;1049::AID-EJSP985&gt;3.0.CO;2-Q</p><a id="c87" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Paolacci, 
              G., 
            Chandler, 
              J., &amp; 
            Ipeirotis, 
              P. G. (
          2010). 
          Running experiments on Amazon Mechanical Turk. 
          <em>Judgment and Decision Making</em>, 
          <em>5</em>, 
          411–
          419.
        </p><a id="c88" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Payne, 
              B. K., 
            Cheng, 
              C. M., 
            Govorun, 
              O., &amp; 
            Stewart, 
              B. D. (
          2005). 
          An inkblot for attitudes: Affect misattribution as implicit measurement. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>89</em>, 
          277–
          293. doi:
          10.1037/0022-3514.89.3.277</p><a id="c89" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Payne, 
              B. K., 
            McClernon, 
              F. J., &amp; 
            Dobbins, 
              I. G. (
          2007). 
          Automatic affective responses to smoking cues. 
          <em>Experimental and Clinical Psychopharmacology</em>, 
          <em>15</em>, 
          400–
          409. doi:
          10.1037/1064-1297.15.4.400</p><a id="c90" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Platow, 
              M. J., 
            Foddy, 
              M., 
            Yamagishi, 
              T., 
            Lim, 
              L., &amp; 
            Chow, 
              A. (
          2012). 
          Two experimental tests of trust in in-group strangers: The moderating role of common knowledge of group membership. 
          <em>European Journal of Social Psychology</em>, 
          <em>42</em>, 
          30–
          35. doi:
          10.1002/ejsp.852</p><a id="c91" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Quanty, 
              M. B., 
            Keats, 
              J. A., &amp; 
            Harkins, 
              S. G. (
          1975). 
          Prejudice and criteria for identification of ethnic photographs. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>32</em>, 
          449–
          454. doi:
          10.1037/h0077093</p><a id="c92" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ratner, 
              K. G., &amp; 
            Amodio, 
              D. M. (
          2013). 
          Seeing “us vs. them”: Minimal group effects on the neural encoding of faces. 
          <em>Journal of Experimental Social Psychology</em>, 
          <em>49</em>, 
          298–
          301. doi:
          10.1016/j.jesp.2012.10.017</p><a id="c93" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ratner, 
              K. G., 
            Kaul, 
              C., &amp; 
            Van Bavel, 
              J. J. (
          2013). 
          Is race erased? Decoding race from patterns of neural activity when skin color is not diagnostic of group boundaries. 
          <em>Social Cognitive and Affective Neuroscience</em>, 
          <em>8</em>, 
          750–
          755. doi:
          10.1093/scan/nss063</p><a id="c94" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Ringach, 
              D., &amp; 
            Shapley, 
              R. (
          2004). 
          Reverse correlation in neurophysiology. 
          <em>Cognitive Science</em>, 
          <em>28</em>, 
          147–
          166. doi:
          10.1207/s15516709cog2802_2</p><a id="c97" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Schacter, 
              D. L., 
            Addis, 
              D. R., &amp; 
            Buckner, 
              R. L. (
          2007). 
          Remembering the past to imagine the future: The prospective brain. 
          <em>Nature Reviews Neuroscience</em>, 
          <em>8</em>, 
          657–
          661. doi:
          10.1038/nrn2213</p><a id="c98" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Schul, 
              Y., 
            Mayo, 
              R., &amp; 
            Burnstein, 
              E. (
          2004). 
          Encoding under trust and distrust: The spontaneous activation of incongruent cognitions. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>86</em>, 
          668–
          679. doi:
          10.1037/0022-3514.86.5.668</p><a id="c99" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Solomon, 
              J. A. (
          2002). 
          Noise reveals visual mechanisms of detection and discrimination. 
          <em>Journal of Vision</em>, 
          <em>2</em>, 
          105–
          120. doi:
          10.1167/2.1.7</p><a id="c100" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Spencer, 
              S. J., 
            Zanna, 
              M. P., &amp; 
            Fong, 
              G. T. (
          2005). 
          Establishing a causal chain: Why experiments are often more effective than mediational analyses in examining psychological processes. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>89</em>, 
          845–
          851. doi:
          10.1037/0022-3514.89.6.845</p><a id="c101" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Summerfield, 
              C., 
            Egner, 
              T., 
            Greene, 
              M., 
            Koechlin, 
              E., 
            Mangels, 
              J., &amp; 
            Hirsch, 
              J. (
          2006, 
          November24). 
          Predictive codes for forthcoming perception in the frontal cortex. 
          <em>Science</em>, 
          <em>314</em>, 
          1311–
          1314. doi:
          10.1126/science.1132028</p><a id="c102" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Tajfel, 
              H. (
          1970). 
          Experiments in intergroup discrimination. 
          <em>Scientific American</em>, 
          <em>223</em>, 
          96–
          102. doi:
          10.1038/scientificamerican1170-96</p><a id="c103" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Tajfel, 
              H., 
            Billig, 
              M. G., 
            Bundy, 
              R. P., &amp; 
            Flament, 
              C. (
          1971). 
          Social categorization and intergroup behaviour. 
          <em>European Journal of Social Psychology</em>, 
          <em>1</em>, 
          149–
          178. doi:
          10.1002/ejsp.2420010202</p><a id="c104" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Tajfel, 
              H., &amp; 
            Turner, 
              J. C. (
          1986). 
          The social identity theory of inter-group behavior. In 
          S.Worchel &amp; 
            L. W.Austin (
          Eds.), 
          <em>Psychology of intergroup relations</em> (pp. 
          7–
          24). 
          Chicago, IL: 
          Nelson-Hall.
        </p><a id="c105" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Tanis, 
              M., &amp; 
            Postmes, 
              T. (
          2005). 
          A social identity approach to trust: Interpersonal perception, group membership and trusting behaviour. 
          <em>European Journal of Social Psychology</em>, 
          <em>35</em>, 
          413–
          424. doi:
          10.1002/ejsp.256</p><a id="c106" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Taylor, 
              S. E., 
            Pham, 
              L. B., 
            Rivkin, 
              I. D., &amp; 
            Armor, 
              D. A. (
          1998). 
          Harnessing the imagination: Mental simulation, self-regulation, and coping. 
          <em>American Psychologist</em>, 
          <em>53</em>, 
          429–
          439. doi:
          10.1037/0003-066X.53.4.429</p><a id="c107" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Todorov, 
              A., 
            Dotsch, 
              R., 
            Wigboldus, 
              D. H. J., &amp; 
            Said, 
              C. P. (
          2011). 
          Data-driven methods for modeling social perception. 
          <em>Social and Personality Psychology Compass</em>, 
          <em>5</em>, 
          775–
          791. doi:
          10.1111/j.1751-9004.2011.00389.x</p><a id="c108" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Todorov, 
              A., 
            Said, 
              C. P., 
            Engell, 
              A. D., &amp; 
            Oosterhof, 
              N. N. (
          2008). 
          Understanding evaluation of faces on social dimensions. 
          <em>Trends in Cognitive Sciences</em>, 
          <em>12</em>, 
          455–
          460. doi:
          10.1016/j.tics.2008.10.001</p><a id="c109" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Tulving, 
              E. (
          1983). 
          <em>Elements of episodic memory</em>. 
          New York, NY: 
          Oxford University Press.
        </p><a id="c110" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Turner, 
              R. N., &amp; 
            Crisp, 
              R. J. (
          2010). 
          Imagining intergroup contact reduces implicit prejudice. 
          <em>British Journal of Social Psychology</em>, 
          <em>49</em>, 
          129–
          142. doi:
          10.1348/014466609X419901</p><a id="c111" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Turner, 
              R. N., 
            Crisp, 
              R. J., &amp; 
            Lambert, 
              E. (
          2007). 
          Imagining intergroup contact can improve intergroup attitudes. 
          <em>Group Processes &amp; Intergroup Relations</em>, 
          <em>10</em>, 
          427–
          441. doi:
          10.1177/1368430207081533</p><a id="c112" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Van Bavel, 
              J. J., &amp; 
            Cunningham, 
              W. A. (
          2009). 
          Self-categorization with a novel mixed-race group moderates automatic social and racial biases. 
          <em>Personality and Social Psychology Bulletin</em>, 
          <em>35</em>, 
          321–
          335. doi:
          10.1177/0146167208327743</p><a id="c113" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Van Bavel, 
              J. J., 
            Packer, 
              D. J., &amp; 
            Cunningham, 
              W. A. (
          2008). 
          The neural substrates of in-group bias: A functional magnetic resonance imaging investigation. 
          <em>Psychological Science</em>, 
          <em>19</em>, 
          1131–
          1139. doi:
          10.1111/j.1467-9280.2008.02214.x</p><a id="c114" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Van Bavel, 
              J. J., 
            Packer, 
              D. J., &amp; 
            Cunningham, 
              W. A. (
          2011). 
          Modulation of the fusiform face area following minimal exposure to motivationally relevant faces: Evidence of in-group enhancement (not out-group disregard). 
          <em>Journal of Cognitive Neuroscience</em>, 
          <em>23</em>, 
          3343–
          3354. doi:
          10.1162/jocn_a_00016</p><a id="c115" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Victor, 
              J. D. (
          2005). 
          Analyzing receptive fields, classification images and functional images: Challenges with opportunities for synergy. 
          <em>Nature Neuroscience</em>, 
          <em>8</em>, 
          1651–
          1656. doi:
          10.1038/nn1607</p><a id="c116" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Whalen, 
              P. J., 
            Rauch, 
              S. L., 
            Etcoff, 
              N. L., 
            McInerney, 
              S. C., 
            Lee, 
              M. B., &amp; 
            Jenike, 
              M. A. (
          1998). 
          Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. 
          <em>Journal of Neuroscience</em>, 
          <em>18</em>, 
          411–
          418.
        </p><a id="c117" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Young, 
              A. I., 
            Ratner, 
              K. G., &amp; 
            Fazio, 
              R. H. (
          2014). 
          Political attitudes bias the mental representation of a presidential candidate’s face. 
          <em>Psychological Science</em>, 
          <em>25</em>, 
          503–
          510. doi:
          10.1177/0956797613510717</p><a id="c118" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Young, 
              S. G., &amp; 
            Hugenberg, 
              K. (
          2010). 
          Mere social categorization modulates identification of facial expressions of emotion. 
          <em>Journal of Personality and Social Psychology</em>, 
          <em>99</em>, 
          964–
          977. doi:
          10.1037/a0020400</p><a id="c119" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Zebrowitz, 
              L. A. (
          1997). 
          <em>Reading faces: Window to the soul?</em>Boulder, CO: 
          Westview Press.
        </p><a id="c120" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Zebrowitz, 
              L. A. (
          2006). 
          Finally, faces find favor. 
          <em>Social Cognition</em>, 
          <em>24</em>, 
          657–
          701. doi:
          10.1521/soco.2006.24.5.657</p><a id="c121" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation">Zebrowitz, 
              L. A., &amp; 
            Montepare, 
              J. M. (
          2008). 
          Social psychological face perception: Why appearance matters. 
          <em>Social and Personality Psychology Compass</em>, 
          <em>2</em>, 
          1497–
          1517. doi:
          10.1111/j.1751-9004.2008.00109.x</p><p class="body-paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Submitted: </em>August 27, 2013<em> Revised: </em>February 7, 2014<em> Accepted: </em>February 10, 2014</p><hr noshade="noshade" /><p class="body-paragraph">This publication is protected by US and international copyright laws and its content may not be copied without the copyright holders express written permission except for the print or download capabilities of the retrieval software used for access. This content is intended solely for the use of the individual user.<br /><br /><strong>Source:&nbsp;</strong>Journal of Personality and Social Psychology. Vol.106 (6) US : American Psychological Association pp. 897-911.<br /><strong>Accession Number:&nbsp;</strong>2014-19904-003 <strong>Digital Object Identifier:&nbsp;</strong>10.1037/a0036498</p></section></div>
	
	

	<div class="content-footer" >
	 

	</div>
	
	<div class="rs-placeholder" id="ctl00_ctl00_MainContentArea_MainContentArea_speaker_box" style="display:none;" data-parent="textToSpeechPlaceholder" data-readid="TextToSpeech" data-speed="MEDIUM" data-voice="ScanSoft_Jill_Full_22kHz" data-server="http://app.rs.ebscohost.com/cgi-bin/rsent?customerid=5845" data-imagepath="http://smallcontent.ebsco-content.com/interfacefiles/14.3.0.380.2/" data-download="true" data-isdetail="true"> </div>



						</div>
					</div>
					<div id="column1" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content">
		
	


<h3 class="vis-hidden">View:</h3>
<ul class="format-control" >
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_listItem" class="format-item active">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_label" title="Detailed Record" class="record-type format-citation">Detailed Record</span>
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_linkButton" title="HTML Full Text" class="record-type html-ft" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl02$linkButton&#39;,&#39;&#39;)">HTML Full Text</a>
			
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_hddnInstructionMessage" class="hidden">This PDF document opens in a frame, to view the document outside of a frame, please change your Adobe Reader settings. To do this, open Adobe Reader, go to Help Menu and select Accessibility Setup Assistant option then select Use Recommend Settings and Skip Setup. You only need to do this once with the current computer you are using.</span>
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_linkButton" title="PDF Full Text" class="record-type pdf-ft" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl03$linkButton&#39;,&#39;&#39;)">PDF Full Text</a>
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_suffix" class="format-note">(334K)</span>
		</li>
	</ul>
	

	
	<div id="citedExternalSources">
		
	</div>
	
		<h3 class="hidden">Cited References</h3>
		<ul class="reference-links">
	
		<li>
			<span id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLinkContainer">				
				<a id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLinkCitation" class="marc-link" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$referencebuttoncontrol$referenceButtonRepeater$ctl01$ReferenceLinkCitation&#39;,&#39;&#39;)">Cited References</a>				
				<a id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLink" class="marc-link" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$referencebuttoncontrol$referenceButtonRepeater$ctl01$ReferenceLink&#39;,&#39;&#39;)">(120)</a>				
			</span>			
			
		</li>
	</ul>

	
	
	
	
	</div>
</div>
					<div role="complementary" id="column2" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content">
		
	<hr class="vis-none" />
<h2 title="Tools" accesskey="5" tabindex="0" class="article-tools-header" id="ArticleTools"  >Tools</h2>
<ul class="article-tools delivery-control">
	<li class="article-tool">
		<a href="#" title="Print" class="print-link"    data-panel='{"Id":"print","Url":"delivery/printpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Print</a>
	</li>
	<li class="article-tool">
		<a href="#" title="E-mail" class="email-link"    data-panel='{"Id":"email","Url":"delivery/emailpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >E-mail</a>
	</li>
	<li class="article-tool">
		<a href="#" title="Save" class="save-link"    data-panel='{"Id":"save","Url":"delivery/savepanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Save</a>
	</li>
	<li class="article-tool">
		<a href="#" title="Cite" class="cite-link"    data-panel='{"Id":"cite","Url":"delivery/citepanel","Js":"ep/controller/control/citepanel.js"}' >Cite</a>
	</li>
	<li class="article-tool">
		<a href="#" title="Export" class="export-link"    data-panel='{"Id":"export","Url":"/ehost/delivery/exportpanel?sid=9c5aba2c-c38b-4471-a994-544fbf07c11d@sessionmgr111\u0026vid=0\u0026form=False","Js":"ep/controller/control/exportpanel.js"}' >Export</a>
	</li>
	<li class="article-tool">
		<a href="#" title="Permalink" class="permalink-link"    data-panel='{"Id":"permalink","Url":"delivery/permalinkpanel","Js":"ep/controller/control/plinkpanel.js"}' >Permalink</a>
	</li>
	<li class="article-tool">
		<a href="#" title="Share" class="bookmark-link"    data-panel='{"Id":"bookmark","Url":"addthis/addthispanel","Js":"ep/controller/control/bookmarkpanel.js"}' >Share</a>
	</li>
	<li class="article-tool">
		<a href="#TextToSpeech" title="Listen" class="listen-link"    >Listen</a>
	</li>
	<li class="article-tool">
		<a href="#Translate" title="Translate" class="translate-link"    >Translate</a>
	</li>
</ul>

	</div>
</div>
				
					
				
				<div class="extra1" role="presentation">&nbsp;</div>
			</div>
			<div class="footer-wrapper" >
				
	

				<div class="push-sticky-footer"></div>
			</div>
		</div>
		
	

				
		


	</form>
	
</body>
</html>
